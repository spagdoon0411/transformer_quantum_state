import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from model.pos_encoding import TQSPositionalEncoding1D, TQSPositionalEncoding2D
from jaxtyping import Float
from model.custom_transformer_layer import TransformerEncoderLayer
from torch.nn import TransformerEncoder


class TransformerModel(nn.Module):
    def __init__(
        self,
        n_dim: int,
        param_dim: int,
        embedding_size: int,
        n_head: int,
        n_hid: int,
        n_layers: int,
        possible_spin_vals: int,
        dropout_encoding: float = 0,
        dropout_transformer: float = 0,
        chunk_size: int | None = None,
    ):
        """
        size_dim: int
            The number of dimensions of the system. For instance, a 1D TFIM has size_dim=1.
            A 2D TFIM has size_dim=2.
        param_dim: int
            The dimension of the physical parameter space. For instance, with a TFIM with
            its only parameter being the magnetic field h, this is 1.
        embedding_size: int
            The dimension of the embedding space that the representation of a site
            within the transformer will have.
        n_head: int
            The number of heads in the multi-head attention mechanism.
        n_hid: int
            The number of hidden units in the feedforward neural network of each
            transformer block.
        n_layers: int
            The number of times the base transformer block (see Attention Is All You Need)
            is applied to the model's input after embedding and before softmax.
        possible_spin_vals: int
            The number of states a single site can be in. In the case of spin-1/2 Fermions,
            this is 2.
        dropout_encoding: float
            The dropout rate used after position encoding is applied to the initial site
            embedding and before the transformer blocks are applied.
        dropout_transformer: float
            The dropout rate used after each transformer block.
        chunk_size: int | None
            The size used to chunk a batch of inputs into to limit memory usage during
            inference. Note that this is not a batch size used during training; it is
            used entirely inside of a forward pass, and computation graphs will be stored
            for each chunk simultaneously and backpropagated through together.
        """

        super(TransformerModel, self).__init__()

        # Constants associated with hyperparameters
        self.embedding_size: int = embedding_size
        self.n_head: int = n_head
        self.n_hid: int = n_hid
        self.n_layers: int = n_layers
        self.dropout_encoding: float = dropout_encoding
        self.dropout_transformer: float = dropout_transformer
        self.chunk_size: int | None = chunk_size

        # Constants that are sufficient for defining a system
        self.n_dim: int = n_dim
        self.param_dim: int = param_dim
        self.possible_spin_vals: int = possible_spin_vals

        # The dimension of the encoding space that parameters and spins are represented
        # in prior to being fed into any trainable layers.
        self.input_dim = self.possible_spin_vals + self.n_dim + 2 + self.param_dim

        # The length of the prefix of the sequence that holds the physical parameters
        # of the system.
        self.seq_prefix_len = self.param_dim + self.n_dim

        # TRAINABLE LAYERS

        # The trainable linear map that represents sites in isolation of one another
        self.encoder = nn.Linear(self.input_dim, self.embedding_size)

        # The positional encoding layer that adds information about the position of a site
        # to the initial isolated representation generated by the encoder.
        match self.n_dim:
            case 1:
                self.pos_encoder = TQSPositionalEncoding1D(
                    self.embedding_size,
                    self.seq_prefix_len,
                    dropout=self.dropout_encoding,
                )
            case 2:
                self.pos_encoder = TQSPositionalEncoding2D(
                    self.embedding_size,
                    self.seq_prefix_len,
                    dropout=self.dropout_encoding,
                )
            case _:
                raise ValueError("Only 1D and 2D systems are supported.")

        # A single encoder block, a la Attention Is All You Need
        encoder_layer = TransformerEncoderLayer(
            self.embedding_size,
            self.n_head,
            self.n_hid,
            dropout=self.dropout_transformer,
        )
        # A mask for the encoder above, turning it into a decoder. Generated on the fly
        # in the forward pass.
        self.src_mask = None
        self.transformer_encoder = TransformerEncoder(encoder_layer, self.n_layers)  # type: ignore

        # The head that interprets generic site logits as wave function probability logits
        self.amp_head = nn.Linear(self.embedding_size, self.possible_spin_vals)

        # The head that interprets generic site logits as wave function phase (angle/argument)
        # logits
        self.phase_head = nn.Linear(self.embedding_size, self.possible_spin_vals)

        self.init_weights()

    def init_weights(self):
        """
        Defines how the initial encoder, amplitude heads, and phase heads are initialized.
        """
        initrange = 0.1
        nn.init.uniform_(self.encoder.weight, -initrange, initrange)
        nn.init.zeros_(self.encoder.bias)
        nn.init.uniform_(self.amp_head.weight, -initrange, initrange)
        nn.init.zeros_(self.amp_head.bias)
        nn.init.uniform_(self.phase_head.weight, -initrange, initrange)
        nn.init.zeros_(self.phase_head.bias)

    def write_params_to_prefix(
        self, values, prefix_encoding, n_dim, phys_dim, n_params, batch_size
    ):
        """
        Parameters:
            values: torch.Tensor (batch_size, n_params)
                The parameter values to write, where each row is a point in parameter space.
            prefix_encoding: torch.Tensor (prefix_dim, batch, input_dim)
                The prefix or input encoding tensor to write to
            n_dim: int
                The number of physical dimensions of the system
            phys_dim: int
                The number of possible values for each site in the chain
            n_params: int
                The number of parameters of the Hamiltonian
            batch_size: int
                The number of points in parameter space that this batch includes
        """

        # Expand each parameter row to a diagonal matrix
        values_diag = torch.diag_embed(values)

        # Identify the prefix_dim-input_dim slice to write to (note that the parameters are
        # written across the entire batch dimension)
        prefix_dim_start = n_dim
        prefix_dim_end = prefix_dim_start + n_params
        input_dim_start = phys_dim + n_dim + 2  # This is param_offset
        input_dim_end = input_dim_start + n_params

        # Write the parameter values to the prefix encoding tensor
        prefix_encoding[
            prefix_dim_start:prefix_dim_end, :batch_size, input_dim_start:input_dim_end
        ] = values_diag.swapaxes(0, 1)

        return prefix_encoding

    def wrap_spins_batch(self, params, spins, system_size):
        """

        Parameters:
            params: torch.Tensor (batch, n_params)
                The parameter values to write, where each row is a point in parameter space.
            spins: torch.Tensor (n, batch_size)
                The spin configurations to write, where each column is a point in the dataset.
            system_size: torch.Tensor (n_dim, )
                The number of sites in the system, along each physical dimension
        """

        n_params = params.shape[1]
        n_dim = system_size.shape[0]

        input_dim = self.possible_spin_vals + n_dim + 2 + n_params
        prefix_len = n_dim + n_params

        n, batch_size = spins.shape
        seq_encoding = torch.zeros(prefix_len + n, batch_size, input_dim)

        size_input = torch.diag(system_size.log())
        parity = system_size % 2

        seq_encoding[
            :n_dim, :, self.possible_spin_vals : (self.possible_spin_vals + n_dim)
        ] = size_input.unsqueeze(1)

        seq_encoding[:n_dim, :, self.possible_spin_vals + n_dim] = parity.unsqueeze(1)

        seq_encoding = self.write_params_to_prefix(
            params, seq_encoding, n_dim, self.possible_spin_vals, n_params, batch_size
        )

        seq_encoding[prefix_len:, :, : self.possible_spin_vals] = (
            torch.nn.functional.one_hot(
                spins.to(torch.int64), num_classes=self.possible_spin_vals
            )
        )

        return seq_encoding

    @staticmethod
    def _generate_square_subsequent_mask(sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = (
            mask.float()
            .masked_fill(mask == 0, float("-inf"))
            .masked_fill(mask == 1, float(0.0))
        )
        return mask

    @staticmethod
    def softsign(x):
        """
        Defined in Hibat-Allah, Mohamed, et al.
                    "Recurrent neural network wave functions."
                    Physical Review Research 2.2 (2020): 023358.
        Used as the activation function on the phase output
        range: (-2pi, 2pi)
        NOTE: this function outputs 2\phi, where \phi is the phase
              an additional factor of 2 is included, to ensure \phi\in(-\pi, \pi)
        """
        return 2 * torch.pi * (1 + x / (1 + x.abs()))

    def forward(
        self,
        params: Float[torch.Tensor, "batch n_params"],
        spins: Float[torch.Tensor, "n batch"],
        system_size: Float[torch.Tensor, "n_dim"],
        compute_phase=True,
    ):
        """
        Forward pass for the model, supporting batching of systems that do not necessarily
        share the same physical parameters but have the same system size.
        """

        src = self.wrap_spins_batch(params, spins, system_size)
        src = self.encoder(src) * math.sqrt(self.embedding_size)

        if self.src_mask is None or self.src_mask.size(0) != len(src):
            mask = self._generate_square_subsequent_mask(len(src)).to(src.device)
            self.src_mask = mask

        src = self.pos_encoder(src, system_size)
        output = self.transformer_encoder(src, self.src_mask)
        psi_output = output[self.seq_prefix_len - 1 :]

        amp = F.log_softmax(self.amp_head(psi_output), dim=-1)

        if compute_phase:
            phase = self.softsign(self.phase_head(psi_output))

        return amp, phase
