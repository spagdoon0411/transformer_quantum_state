import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from model.pos_encoding import TQSPositionalEncoding1D, TQSPositionalEncoding2D
from jaxtyping import Float
from model.custom_transformer_layer import TransformerEncoderLayer
from torch.nn import TransformerEncoder
import numpy as np


class TransformerModel(nn.Module):
    def __init__(
        self,
        n_dim: int,
        param_dim: int,
        embedding_size: int,
        n_head: int,
        n_hid: int,
        n_layers: int,
        possible_spin_vals: int,
        compat_dict: dict = None,
        dropout_encoding: float = 0,
        dropout_transformer: float = 0,
        chunk_size: int | None = None,
        minibatch: int | None = 10000,
    ):
        """
        size_dim: int
            The number of dimensions of the system. For instance, a 1D TFIM has size_dim=1.
            A 2D TFIM has size_dim=2.
        param_dim: int
            The dimension of the physical parameter space. For instance, with a TFIM with
            its only parameter being the magnetic field h, this is 1.
        embedding_size: int
            The dimension of the embedding space that the representation of a site
            within the transformer will have.
        n_head: int
            The number of heads in the multi-head attention mechanism.
        n_hid: int
            The number of hidden units in the feedforward neural network of each
            transformer block.
        n_layers: int
            The number of times the base transformer block (see Attention Is All You Need)
            is applied to the model's input after embedding and before softmax.
        possible_spin_vals: int
            The number of states a single site can be in. In the case of spin-1/2 Fermions,
            this is 2.
        compat_dict: dict | None
            A dictionary containing keys "system_sizes", "param_range", used for compatibility
            with the old model behavior but not used in the new forward batching behavior
            (which takes a tensor of parameters and a system size in its forward pass). If None,
            the model will not be compatible with the old behavior.
        dropout_encoding: float
            The dropout rate used after position encoding is applied to the initial site
            embedding and before the transformer blocks are applied.
        dropout_transformer: float
            The dropout rate used after each transformer block.
        chunk_size: int | None
            The size used to chunk a batch of inputs into to limit memory usage during
            inference. Note that this is not a batch size used during training; it is
            used entirely inside of a forward pass, and computation graphs will be stored
            for each chunk simultaneously and backpropagated through together.
        minibatch: int | None
            Only considered when using forward_classic.
        """

        super(TransformerModel, self).__init__()

        # Constants associated with hyperparameters
        self.embedding_size: int = embedding_size
        self.n_head: int = n_head
        self.n_hid: int = n_hid
        self.n_layers: int = n_layers
        self.dropout_encoding: float = dropout_encoding
        self.dropout_transformer: float = dropout_transformer
        self.chunk_size: int | None = chunk_size

        # Constants that are sufficient for defining a system
        self.n_dim: int = n_dim
        self.param_dim: int = param_dim
        self.possible_spin_vals: int = possible_spin_vals

        # Variables held for backwards-compatibility with the classic forward pass
        # and classic model behavior
        if compat_dict is not None:
            self.system_sizes = compat_dict["system_sizes"]
            self.system_size = None
            self.param_range = compat_dict["param_range"]
            self.param = None
            self.minibatch = minibatch
            self.phys_dim = self.possible_spin_vals

        # The dimension of the encoding space that parameters and spins are represented
        # in prior to being fed into any trainable layers.
        self.input_dim = self.possible_spin_vals + self.n_dim + 2 + self.param_dim

        # The length of the prefix of the sequence that holds the physical parameters
        # of the system.
        self.seq_prefix_len = self.param_dim + self.n_dim

        # TRAINABLE LAYERS

        # The trainable linear map that represents sites in isolation of one another
        self.encoder = nn.Linear(self.input_dim, self.embedding_size)

        # The positional encoding layer that adds information about the position of a site
        # to the initial isolated representation generated by the encoder.
        match self.n_dim:
            case 1:
                self.pos_encoder = TQSPositionalEncoding1D(
                    self.embedding_size,
                    self.seq_prefix_len,
                    dropout=self.dropout_encoding,
                )
            case 2:
                self.pos_encoder = TQSPositionalEncoding2D(
                    self.embedding_size,
                    self.seq_prefix_len,
                    dropout=self.dropout_encoding,
                )
            case _:
                raise ValueError("Only 1D and 2D systems are supported.")

        # A single encoder block, a la Attention Is All You Need
        encoder_layer = TransformerEncoderLayer(
            self.embedding_size,
            self.n_head,
            self.n_hid,
            dropout=self.dropout_transformer,
        )
        # A mask for the encoder above, turning it into a decoder. Generated on the fly
        # in the forward pass.
        self.src_mask = None
        self.transformer_encoder = TransformerEncoder(encoder_layer, self.n_layers)  # type: ignore

        # The head that interprets generic site logits as wave function probability logits
        self.amp_head = nn.Linear(self.embedding_size, self.possible_spin_vals)

        # The head that interprets generic site logits as wave function phase (angle/argument)
        # logits
        self.phase_head = nn.Linear(self.embedding_size, self.possible_spin_vals)

        self.init_weights()

    def init_weights(self):
        """
        Defines how the initial encoder, amplitude heads, and phase heads are initialized.
        """
        initrange = 0.1
        nn.init.uniform_(self.encoder.weight, -initrange, initrange)
        nn.init.zeros_(self.encoder.bias)
        nn.init.uniform_(self.amp_head.weight, -initrange, initrange)
        nn.init.zeros_(self.amp_head.bias)
        nn.init.uniform_(self.phase_head.weight, -initrange, initrange)
        nn.init.zeros_(self.phase_head.bias)

    def write_params_to_prefix(
        self, values, prefix_encoding, n_dim, phys_dim, n_params, batch_size
    ):
        """
        Parameters:
            values: torch.Tensor (batch_size, n_params)
                The parameter values to write, where each row is a point in parameter space.
            prefix_encoding: torch.Tensor (prefix_dim, batch, input_dim)
                The prefix or input encoding tensor to write to
            n_dim: int
                The number of physical dimensions of the system
            phys_dim: int
                The number of possible values for each site in the chain
            n_params: int
                The number of parameters of the Hamiltonian
            batch_size: int
                The number of points in parameter space that this batch includes
        """

        # Expand each parameter row to a diagonal matrix
        values_diag = torch.diag_embed(values)

        # Identify the prefix_dim-input_dim slice to write to (note that the parameters are
        # written across the entire batch dimension)
        prefix_dim_start = n_dim
        prefix_dim_end = prefix_dim_start + n_params
        input_dim_start = phys_dim + n_dim + 2  # This is param_offset
        input_dim_end = input_dim_start + n_params

        # Write the parameter values to the prefix encoding tensor
        prefix_encoding[
            prefix_dim_start:prefix_dim_end, :batch_size, input_dim_start:input_dim_end
        ] = values_diag.swapaxes(0, 1)

        return prefix_encoding

    def set_param(self, system_size=None, param=None):
        """
        Kept for compatibility with the old model behavior. Does not affect the new forward
        pass.
        """
        n_size = self.system_sizes.shape[0]
        size_idx = torch.randint(n_size, [])
        if system_size is None:
            self.system_size = self.system_sizes[size_idx]
        else:
            self.system_size = system_size
        if param is None:
            self.param = self.param_range[0] + torch.rand(self.param_dim) * (
                self.param_range[1] - self.param_range[0]
            )
        else:
            self.param = param
        # self.prefix = self.init_seq()

    def wrap_spins_batch(self, params, spins, system_size):
        """

        Parameters:
            params: torch.Tensor (batch, n_params)
                The parameter values to write, where each row is a point in parameter space.
            spins: torch.Tensor (n, batch_size)
                The spin configurations to write, where each column is a point in the dataset.
            system_size: torch.Tensor (n_dim, )
                The number of sites in the system, along each physical dimension
        """

        n_params = params.shape[1]
        n_dim = system_size.shape[0]

        input_dim = self.possible_spin_vals + n_dim + 2 + n_params
        prefix_len = n_dim + n_params

        n, batch_size = spins.shape
        seq_encoding = torch.zeros(prefix_len + n, batch_size, input_dim)

        size_input = torch.diag(system_size.log())
        parity = system_size % 2

        seq_encoding[
            :n_dim, :, self.possible_spin_vals : (self.possible_spin_vals + n_dim)
        ] = size_input.unsqueeze(1)

        seq_encoding[:n_dim, :, self.possible_spin_vals + n_dim] = parity.unsqueeze(1)

        seq_encoding = self.write_params_to_prefix(
            params, seq_encoding, n_dim, self.possible_spin_vals, n_params, batch_size
        )

        seq_encoding[prefix_len:, :, : self.possible_spin_vals] = (
            torch.nn.functional.one_hot(
                spins.to(torch.int64), num_classes=self.possible_spin_vals
            )
        )

        return seq_encoding

    @staticmethod
    def _generate_square_subsequent_mask(sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = (
            mask.float()
            .masked_fill(mask == 0, float("-inf"))
            .masked_fill(mask == 1, float(0.0))
        )
        return mask

    @staticmethod
    def softsign(x):
        """
        Defined in Hibat-Allah, Mohamed, et al.
                    "Recurrent neural network wave functions."
                    Physical Review Research 2.2 (2020): 023358.
        Used as the activation function on the phase output
        range: (-2pi, 2pi)
        NOTE: this function outputs 2\phi, where \phi is the phase
              an additional factor of 2 is included, to ensure \phi\in(-\pi, \pi)
        """
        return 2 * torch.pi * (1 + x / (1 + x.abs()))

    def forward_batched(
        self,
        params: Float[torch.Tensor, "batch n_params"],  # TODO: verify dims
        spins: Float[torch.Tensor, "n batch"],
        system_size: Float[torch.Tensor, "n_dim"],
        compute_phase=True,
    ):
        """
        Forward pass for the model, supporting batching of systems that do not necessarily
        share the same physical parameters but have the same system size.
        """

        src = self.wrap_spins_batch(params, spins, system_size)
        src = self.encoder(src) * math.sqrt(self.embedding_size)

        if self.src_mask is None or self.src_mask.size(0) != len(src):
            mask = self._generate_square_subsequent_mask(len(src)).to(src.device)
            self.src_mask = mask

        src = self.pos_encoder(src, system_size)
        output = self.transformer_encoder(src, self.src_mask)
        psi_output = output[self.seq_prefix_len - 1 :]

        amp = F.log_softmax(self.amp_head(psi_output), dim=-1)

        if compute_phase:
            phase = self.softsign(self.phase_head(psi_output))

        return amp, phase

    def forward(self, spins, compute_phase=True):
        # src: (seq, batch, input_dim)
        # use_symmetry: has no effect in this function
        # only included to be consistent with the symmetric version

        # One-hot encode the spins
        # TODO: is this exactly what wrap_spins_batch expects?

        if self.system_size is None or self.param is None:
            raise ValueError(
                "System size and parameter must be set before forward pass. Use set_param, a method meant for compatibility with the original model structure."
            )

        params = self.param.unsqueeze(0).expand(spins.shape[1], -1)

        src = self.wrap_spins_batch(params, spins, self.system_size)

        # src = self.wrap_spins(spins)
        if self.src_mask is None or self.src_mask.size(0) != len(src):
            mask = self._generate_square_subsequent_mask(len(src)).to(src.device)
            self.src_mask = mask

        system_size = src[
            : self.n_dim,
            0,
            self.possible_spin_vals : self.possible_spin_vals + self.n_dim,
        ].diag()  # (n_dim, )
        system_size = system_size.exp().round().to(torch.int64)  # (n_dim, )

        result = []
        if self.minibatch is None:
            # Map the one-hot encoded spins to an initial embedding with a
            # trainable linear transformation. TODO: Why do we not divide?
            src = self.encoder(src) * math.sqrt(
                self.embedding_size
            )  # (seq, batch, embedding)
            # src = src + self.pos_embedding[:len(src)]  # (seq, batch, embedding)

            # Perform the parameter and spin positional embedding, adding position
            # information to this particular sequence of parameters, then to spins
            src = self.pos_encoder(src, system_size)  # (seq, batch, embedding)

            # Pass the embedded spins through the transformer encoder
            output = self.transformer_encoder(
                src, self.src_mask
            )  # (seq, batch, embedding)

            # Retrieve only the parts of the output sequence that
            # correspond to the wave function conditional probabilities
            psi_output = output[
                self.seq_prefix_len - 1 :
            ]  # only use the physical degrees of freedom

            # Apply a trainable linear transformation (self.amp_head) to produce
            # logits for the conditional probabilities of the wave function. Apply
            # softmax after that to get the actual probabilities.
            amp = F.log_softmax(
                self.amp_head(psi_output), dim=-1
            )  # (n, batch, phys_dim)

            result.append(amp)

            # Do something similar for phases, but compute the softsign function
            # instead of softmax
            if compute_phase:
                phase = self.softsign(
                    self.phase_head(psi_output)
                )  # (seq, batch, phys_dim)
                result.append(phase)
        else:
            batch = src.shape[1]
            minibatch = self.minibatch
            repeat = int(np.ceil(batch / minibatch))
            amp = []
            phase = []
            for i in range(repeat):
                src_i = src[:, i * minibatch : (i + 1) * minibatch]
                src_i = self.encoder(src_i) * math.sqrt(
                    self.embedding_size
                )  # (seq, batch, embedding)
                # src_i = src_i + self.pos_embedding[:len(src_i)]  # (seq, batch, embedding)
                src_i = self.pos_encoder(src_i, system_size)  # (seq, batch, embedding)
                output_i = self.transformer_encoder(
                    src_i, self.src_mask
                )  # (seq, batch, embedding)
                psi_output = output_i[
                    self.seq_prefix_len - 1 :
                ]  # only use the physical degrees of freedom
                amp_i = F.log_softmax(
                    self.amp_head(psi_output), dim=-1
                )  # (seq, batch, phys_dim)
                amp.append(amp_i)
                if compute_phase:
                    phase_i = self.softsign(
                        self.phase_head(psi_output)
                    )  # (seq, batch, phys_dim)
                    phase.append(phase_i)
            amp = torch.cat(amp, dim=1)
            result.append(amp)
            if compute_phase:
                phase = torch.cat(phase, dim=1)
                result.append(phase)
        return result
