{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_path = \"/Users/spandan/Projects/transformer_quantum_state/\"\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from hamiltonians.Ising import Ising\n",
    "\n",
    "from model.model_batched import TransformerModel\n",
    "from optimizers.optimizer_supervised_batches import Optimizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "from cuda_setup import cuda_setup\n",
    "from optimizers.bookkeeping_tools import generate_monitor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU unavailable; using CPU\n"
     ]
    }
   ],
   "source": [
    "cuda_setup()\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hamiltonians and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the dataset to create in TFIM_ground_states\n",
    "dataset_dir_name = \"h_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]0;Julia\u0007\u001b]0;Julia\u0007\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[?25l\u001b[?25h\u001b[2K\u001b[?25h\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[?25l\u001b[?25h\u001b[2K\u001b[?25h\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[?25l\u001b[?25h\u001b[2K\u001b[?25h\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[?25l\u001b[?25h\u001b[2K\u001b[?25h\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[?25l\u001b[?25h\u001b[2K\u001b[?25hSwitching to N = 2\n",
      "  0.899622 seconds (367.55 k allocations: 25.024 MiB, 2219.41% compilation time)\n",
      "N = 2, h = 0.5\n",
      "  1.402845 seconds (1.33 M allocations: 74.415 MiB, 2.95% gc time, 99.97% compilation time)\n",
      "Switching to N = 3\n",
      "  0.000853 seconds (415 allocations: 40.250 KiB)\n",
      "N = 3, h = 0.5\n",
      "  0.000067 seconds (60 allocations: 7.203 KiB)\n",
      "Switching to N = 4\n",
      "  0.000822 seconds (491 allocations: 51.594 KiB)\n",
      "N = 4, h = 0.5\n",
      "  0.000088 seconds (73 allocations: 17.484 KiB)\n",
      "Switching to N = 5\n",
      "  0.000981 seconds (587 allocations: 75.375 KiB)\n",
      "N = 5, h = 0.5\n",
      "  0.000071 seconds (78 allocations: 30.922 KiB)\n",
      "Switching to N = 6\n",
      "  0.008842 seconds (703 allocations: 127.250 KiB)\n",
      "N = 6, h = 0.5\n",
      "  0.000215 seconds (90 allocations: 50.969 KiB)\n",
      "Switching to N = 7\n",
      "  0.000879 seconds (839 allocations: 242.344 KiB)\n",
      "N = 7, h = 0.5\n",
      "  0.000352 seconds (108 allocations: 92.125 KiB)\n",
      "Switching to N = 8\n",
      "  0.000725 seconds (995 allocations: 509.406 KiB)\n",
      "N = 8, h = 0.5\n",
      "  0.000448 seconds (107 allocations: 174.406 KiB)\n",
      "Switching to N = 9\n",
      "  0.001273 seconds (1.17 k allocations: 1.071 MiB)\n",
      "N = 9, h = 0.5\n",
      "  0.000510 seconds (104 allocations: 352.234 KiB)\n",
      "Switching to N = 10\n",
      "  0.001101 seconds (1.37 k allocations: 2.362 MiB)\n",
      "N = 10, h = 0.5\n",
      "  0.001299 seconds (122 allocations: 722.094 KiB)\n",
      "Switching to N = 11\n",
      "  0.018924 seconds (1.63 k allocations: 5.240 MiB)\n",
      "N = 11, h = 0.5\n",
      "  0.004367 seconds (142 allocations: 1.457 MiB)\n",
      "Switching to N = 12\n",
      "  0.052809 seconds (1.89 k allocations: 11.680 MiB)\n",
      "N = 12, h = 0.5\n",
      "  0.007175 seconds (140 allocations: 2.990 MiB)\n",
      "Switching to N = 13\n",
      "  0.023955 seconds (2.18 k allocations: 25.945 MiB)\n",
      "N = 13, h = 0.5\n",
      "  0.013487 seconds (139 allocations: 6.268 MiB)\n",
      "Switching to N = 14\n",
      "  0.074080 seconds (2.49 k allocations: 57.480 MiB)\n",
      "N = 14, h = 0.5\n",
      "  0.085452 seconds (160 allocations: 13.020 MiB, 11.69% gc time)\n",
      "Switching to N = 15\n",
      "  0.120189 seconds (2.83 k allocations: 126.672 MiB)\n",
      "N = 15, h = 0.5\n",
      "  0.096343 seconds (171 allocations: 27.021 MiB)\n",
      "Switching to N = 16\n",
      "  0.137657 seconds (3.19 k allocations: 278.132 MiB)\n",
      "N = 16, h = 0.5\n",
      "  0.153769 seconds (157 allocations: 55.627 MiB, 9.62% gc time)\n",
      "Switching to N = 17\n",
      "  0.269233 seconds (3.57 k allocations: 607.620 MiB)\n",
      "N = 17, h = 0.5\n",
      "  0.457716 seconds (169 allocations: 116.020 MiB, 14.35% gc time)\n",
      "Switching to N = 18\n",
      "  0.621570 seconds (3.98 k allocations: 1.292 GiB, 7.22% gc time)\n",
      "N = 18, h = 0.5\n",
      "  0.811317 seconds (169 allocations: 240.020 MiB, 2.62% gc time)\n",
      "Switching to N = 19\n",
      "  1.219009 seconds (4.41 k allocations: 2.801 GiB, 9.69% gc time)\n",
      "N = 19, h = 0.5\n",
      "  2.028032 seconds (181 allocations: 496.021 MiB, 3.09% gc time)\n",
      "Switching to N = 20\n",
      "  2.627473 seconds (4.87 k allocations: 6.055 GiB, 2.85% gc time)\n",
      "N = 20, h = 0.5\n",
      "  2.708858 seconds (118 allocations: 1018.377 MiB, 7.29% gc time)\n"
     ]
    }
   ],
   "source": [
    "!julia --threads 28 ./createdata/create_datasets.jl {dataset_dir_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1719361022906/work/aten/src/ATen/native/TensorShape.cpp:3679.)\n",
      "  return func(*args, **kwargs)\n",
      "/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "system_sizes = torch.arange(4, 4 + 1, 1).reshape(-1, 1)\n",
    "Hamiltonians = [Ising(size, periodic=True, get_basis=True) for size in system_sizes]\n",
    "data_dir_path = os.path.join(\"TFIM_ground_states\", dataset_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for system size 4 from /Users/spandan/Projects/transformer_quantum_state/TFIM_ground_states/h_tiny/4.arrow.\n",
      "(h_min, h_step, h_max) = (0.5, -1, 1.5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1719361022906/work/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for ham in Hamiltonians:\n",
    "    ham.load_dataset(\n",
    "        data_dir_path,\n",
    "        # batch_size=batch_from_n(ham.n),\n",
    "        batch_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dim = Hamiltonians[0].param_dim\n",
    "embedding_size = 32\n",
    "n_head = 8\n",
    "n_hid = embedding_size\n",
    "n_layers = 8\n",
    "dropout = 0\n",
    "minibatch = 10000\n",
    "param_range = None\n",
    "point_of_interest = None\n",
    "use_SR = False\n",
    "\n",
    "compat_dict = {\n",
    "    \"system_sizes\": system_sizes,\n",
    "    \"param_range\": None,\n",
    "}\n",
    "\n",
    "model = TransformerModel(\n",
    "    n_dim=1,\n",
    "    param_dim=param_dim,\n",
    "    embedding_size=embedding_size,\n",
    "    n_head=n_head,\n",
    "    n_hid=n_hid,\n",
    "    n_layers=n_layers,\n",
    "    possible_spin_vals=2,\n",
    "    compat_dict=compat_dict,\n",
    "    dropout_encoding=dropout,\n",
    "    dropout_transformer=dropout,\n",
    "    minibatch=minibatch,\n",
    ").to(device=\"cpu\")\n",
    "\n",
    "results_dir = \"results\"\n",
    "paper_checkpoint_name = \"ckpt_100000_Ising_32_8_8_0.ckpt\"\n",
    "paper_checkpoint_path = os.path.join(results_dir, paper_checkpoint_name)\n",
    "checkpoint = torch.load(paper_checkpoint_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Optimizer and Generalization Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended lr range: 1e-9 to 1e-2\n",
    "opt = Optimizer(\n",
    "    model,\n",
    "    Hamiltonians,\n",
    "    lr=1e-7,\n",
    "    beta1=0.9,\n",
    "    beta2=0.98,\n",
    "    point_of_interest=point_of_interest,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes to monitor:\n",
      " tensor([[15],\n",
      "        [40]])\n",
      "Params to monitor:\n",
      " tensor([[0.6000, 1.0000, 1.4000]])\n"
     ]
    }
   ],
   "source": [
    "monitor_sizes = torch.tensor([15, 40]).reshape(-1, 1)\n",
    "monitor_params = torch.tensor([0.6, 1.0, 1.4]).unsqueeze(0)\n",
    "print(\"Sizes to monitor:\\n\", monitor_sizes)\n",
    "print(\"Params to monitor:\\n\", monitor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System size keys: [[15], [40]]\n",
      "Param keys: [[0.6], [1.0], [1.4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "monitor_dict = generate_monitor_dict(\n",
    "    monitor_sizes=monitor_sizes, monitor_params=monitor_params, epochs_anticipated=30000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMRG Energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.6: -42.87900161743164, 1.0: -50.569435119628906, 1.4: -63.206722259521484}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dmrg_energies(h_values):\n",
    "    drmg40path = os.path.join(\"results\", \"E_dmrg_40.npy\")\n",
    "    dmrg40 = np.load(drmg40path)\n",
    "    dmrg40 = torch.tensor(dmrg40, dtype=torch.float32)\n",
    "\n",
    "    dmrg40_h_values = torch.linspace(0, 2, 101)\n",
    "    energies = {}\n",
    "    for h in h_values:\n",
    "        energy = dmrg40[torch.where(torch.isclose(dmrg40_h_values, torch.tensor(h)))]\n",
    "        energies[h] = energy.item()\n",
    "    return energies\n",
    "\n",
    "\n",
    "dmrg_energies = get_dmrg_energies([0.6, 1.0, 1.4])\n",
    "\n",
    "dmrg_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': tensor([1.]),\n",
       " 'energy': None,\n",
       " 'epoch_errors': [],\n",
       " 'epoch_relative_errors': [],\n",
       " 'epoch_E_mean': [],\n",
       " 'epoch_E_var': [],\n",
       " 'epoch_Er': [],\n",
       " 'epoch_Ei': [],\n",
       " 'epoch_averaged_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_relative_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_mean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_var': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Er': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Ei': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[40]\"][\"params\"][\"[1.0]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_dict[\"[40]\"][\"params\"][\"[0.6]\"][\"energy\"] = dmrg_energies[0.6]\n",
    "monitor_dict[\"[40]\"][\"params\"][\"[1.0]\"][\"energy\"] = dmrg_energies[1.0]\n",
    "monitor_dict[\"[40]\"][\"params\"][\"[1.4]\"][\"energy\"] = dmrg_energies[1.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': tensor([0.6000]),\n",
       " 'energy': -42.87900161743164,\n",
       " 'epoch_errors': [],\n",
       " 'epoch_relative_errors': [],\n",
       " 'epoch_E_mean': [],\n",
       " 'epoch_E_var': [],\n",
       " 'epoch_Er': [],\n",
       " 'epoch_Ei': [],\n",
       " 'epoch_averaged_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_relative_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_mean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_var': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Er': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Ei': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[40]\"][\"params\"][\"[0.6]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': tensor([1.]),\n",
       " 'energy': -50.569435119628906,\n",
       " 'epoch_errors': [],\n",
       " 'epoch_relative_errors': [],\n",
       " 'epoch_E_mean': [],\n",
       " 'epoch_E_var': [],\n",
       " 'epoch_Er': [],\n",
       " 'epoch_Ei': [],\n",
       " 'epoch_averaged_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_relative_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_mean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_var': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Er': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Ei': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[40]\"][\"params\"][\"[1.0]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': tensor([1.4000]),\n",
       " 'energy': -63.206722259521484,\n",
       " 'epoch_errors': [],\n",
       " 'epoch_relative_errors': [],\n",
       " 'epoch_E_mean': [],\n",
       " 'epoch_E_var': [],\n",
       " 'epoch_Er': [],\n",
       " 'epoch_Ei': [],\n",
       " 'epoch_averaged_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_relative_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_mean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_var': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Er': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Ei': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[40]\"][\"params\"][\"[1.4]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-Force Energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_15 = Ising(torch.tensor([15]), periodic=True, get_basis=False)\n",
    "energy_15_0_6 = ham_15.calc_E_ground(param=0.6)\n",
    "energy_15_1_0 = ham_15.calc_E_ground(param=1.0)\n",
    "energy_15_1_4 = ham_15.calc_E_ground(param=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_dict[\"[15]\"][\"params\"][\"[0.6]\"][\"energy\"] = energy_15_0_6\n",
    "monitor_dict[\"[15]\"][\"params\"][\"[1.0]\"][\"energy\"] = energy_15_1_0\n",
    "monitor_dict[\"[15]\"][\"params\"][\"[1.4]\"][\"energy\"] = energy_15_1_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': tensor([0.6000]),\n",
       " 'energy': -16.383636410876687,\n",
       " 'epoch_errors': [],\n",
       " 'epoch_relative_errors': [],\n",
       " 'epoch_E_mean': [],\n",
       " 'epoch_E_var': [],\n",
       " 'epoch_Er': [],\n",
       " 'epoch_Ei': [],\n",
       " 'epoch_averaged_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_relative_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_mean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_var': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Er': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Ei': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[15]\"][\"params\"][\"[0.6]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['system_size', 'H', 'params'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[15]\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['[15]', '[40]'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': tensor([1.]),\n",
       " 'energy': -19.133544467011305,\n",
       " 'epoch_errors': [],\n",
       " 'epoch_relative_errors': [],\n",
       " 'epoch_E_mean': [],\n",
       " 'epoch_E_var': [],\n",
       " 'epoch_Er': [],\n",
       " 'epoch_Ei': [],\n",
       " 'epoch_averaged_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_relative_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_mean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_var': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Er': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Ei': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[15]\"][\"params\"][\"[1.0]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': tensor([1.4000]),\n",
       " 'energy': -23.778807936182726,\n",
       " 'epoch_errors': [],\n",
       " 'epoch_relative_errors': [],\n",
       " 'epoch_E_mean': [],\n",
       " 'epoch_E_var': [],\n",
       " 'epoch_Er': [],\n",
       " 'epoch_Ei': [],\n",
       " 'epoch_averaged_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_relative_errors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_mean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_E_var': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Er': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'epoch_averaged_Ei': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_dict[\"[15]\"][\"params\"][\"[1.4]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use tensorboard --logdir supervised_results/Ising_32_8_8_0_supervised_2024_08_09_07_46_36_414290/tensorboard_logs for monitoring. Pass --bind-all if training remotely.\n",
      "Starting epoch 0\n",
      "\tN = [4]\n",
      "\t\tIter 0 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 1 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 2 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 3 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 4 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 5 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 6 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 7 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 8 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 9 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 10 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 11 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 12 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 13 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 14 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 15 - h ∈ [0.5, 0.5]\n",
      "Starting epoch 1\n",
      "\tN = [4]\n",
      "\t\tIter 0 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 1 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 2 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 3 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 4 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 5 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 6 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 7 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 8 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 9 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 10 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 11 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 12 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 13 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 14 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 15 - h ∈ [0.5, 0.5]\n",
      "Starting epoch 2\n",
      "\tN = [4]\n",
      "\t\tIter 0 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 1 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 2 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 3 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 4 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 5 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 6 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 7 - h ∈ [0.5, 0.5]\n",
      "\t\tIter 8 - h ∈ [0.5, 0.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_tensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/optimizers/optimizer_supervised_batches.py:521\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, monitor_dict, param_range, log_tensorboard, ensemble_id, prob_weight, arg_weight, start_epoch)\u001b[0m\n\u001b[1;32m    507\u001b[0m basis_states, params, psi_true \u001b[38;5;241m=\u001b[39m dataset[sample_idx]\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsume_batch(\n\u001b[1;32m    510\u001b[0m     H,\n\u001b[1;32m    511\u001b[0m     basis_states\u001b[38;5;241m=\u001b[39mbasis_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28miter\u001b[39m\u001b[38;5;241m=\u001b[39mepoch_iter,\n\u001b[1;32m    519\u001b[0m )\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreport_on_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# TODO: can we move this to the consume_batch function\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# without causing issues with plot_grad_flow in report_on_batch?\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/optimizers/optimizer_supervised_batches.py:377\u001b[0m, in \u001b[0;36mOptimizer.report_on_batch\u001b[0;34m(self, monitor_dict, global_iteration, writer)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# If a monitor_dict was passed, report energy estimates according\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# to the self.energy_estimation_iter_frequency\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (monitor_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     global_iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_estimation_iter_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    376\u001b[0m ):\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreport_energy_estimate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/optimizers/optimizer_supervised_batches.py:129\u001b[0m, in \u001b[0;36mOptimizer.report_energy_estimate\u001b[0;34m(self, monitor_dict, iteration, writer, writer_iter, num_samples, max_unique)\u001b[0m\n\u001b[1;32m    126\u001b[0m param \u001b[38;5;241m=\u001b[39m info_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Calculate the energy estimate\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m E_mean, E_var, Er, Ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_energy_estimate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_unique\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# TODO: why is E_mean imaginary?\u001b[39;00m\n\u001b[1;32m    133\u001b[0m E_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(E_mean)\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/optimizers/optimizer_supervised_batches.py:197\u001b[0m, in \u001b[0;36mOptimizer.extract_energy_estimate\u001b[0;34m(self, H, param, num_samples, max_unique)\u001b[0m\n\u001b[1;32m    195\u001b[0m symmetry \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39msymmetry\n\u001b[1;32m    196\u001b[0m samples, sample_weight \u001b[38;5;241m=\u001b[39m sample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, num_samples, max_unique, symmetry)\n\u001b[0;32m--> 197\u001b[0m E \u001b[38;5;241m=\u001b[39m \u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_symmetry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m E_mean \u001b[38;5;241m=\u001b[39m (E \u001b[38;5;241m*\u001b[39m sample_weight)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    199\u001b[0m E_var \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    200\u001b[0m     (((E \u001b[38;5;241m-\u001b[39m E_mean)\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m sample_weight)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m H\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# .numpy()\u001b[39;00m\n\u001b[1;32m    204\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/hamiltonians/Hamiltonian.py:102\u001b[0m, in \u001b[0;36mHamiltonian.Eloc\u001b[0;34m(self, samples, sample_weight, model, use_symmetry)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_param(params)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Hi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH:\n\u001b[0;32m--> 102\u001b[0m     O \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_observable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymmetry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymmetry\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m Oj \u001b[38;5;129;01min\u001b[39;00m O:\n\u001b[1;32m    107\u001b[0m         E \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Oj\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/model_utils.py:404\u001b[0m, in \u001b[0;36mcompute_observable\u001b[0;34m(model, samples, sample_weight, observable, batch_mean, symmetry)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_sites_i\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    403\u001b[0m     flip_idx \u001b[38;5;241m=\u001b[39m spin_idx\u001b[38;5;241m.\u001b[39mT[flip_sites_i]\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# (n_op, n_flip)\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m     psixp_over_psix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_flip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymmetry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_phase\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (n_op, batch)\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     flip_results\u001b[38;5;241m.\u001b[39mappend(psixp_over_psix)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/model_utils.py:473\u001b[0m, in \u001b[0;36mcompute_flip\u001b[0;34m(model, samples, flip_idx, symmetry, log_amp, log_phase)\u001b[0m\n\u001b[1;32m    470\u001b[0m flip_mask[flip_idx, torch\u001b[38;5;241m.\u001b[39marange(n_op)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    471\u001b[0m samples_flipped[flip_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m samples_flipped[flip_mask]\n\u001b[0;32m--> 473\u001b[0m log_amp_1, log_phase_1 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_psi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_flipped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_op\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymmetry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_duplicate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (n_op*batch)\u001b[39;00m\n\u001b[1;32m    476\u001b[0m log_amp_1 \u001b[38;5;241m=\u001b[39m log_amp_1\u001b[38;5;241m.\u001b[39mreshape(n_op, batch)\n\u001b[1;32m    477\u001b[0m log_phase_1 \u001b[38;5;241m=\u001b[39m log_phase_1\u001b[38;5;241m.\u001b[39mreshape(n_op, batch)\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/model_utils.py:227\u001b[0m, in \u001b[0;36mcompute_psi\u001b[0;34m(model, samples, symmetry, check_duplicate, params)\u001b[0m\n\u001b[1;32m    224\u001b[0m spin_idx \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     log_amp, log_phase \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_phase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq, batch, phys_dim)\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     log_amp, log_phase \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_batched(\n\u001b[1;32m    232\u001b[0m         params, samples, model\u001b[38;5;241m.\u001b[39msystem_size, phys_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, compute_phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/model_batched.py:390\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, spins, compute_phase)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# src_i = src_i + self.pos_embedding[:len(src_i)]  # (seq, batch, embedding)\u001b[39;00m\n\u001b[1;32m    389\u001b[0m src_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src_i, system_size)  \u001b[38;5;66;03m# (seq, batch, embedding)\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m output_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_mask\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq, batch, embedding)\u001b[39;00m\n\u001b[1;32m    393\u001b[0m psi_output \u001b[38;5;241m=\u001b[39m output_i[\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_prefix_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m :\n\u001b[1;32m    395\u001b[0m ]  \u001b[38;5;66;03m# only use the physical degrees of freedom\u001b[39;00m\n\u001b[1;32m    396\u001b[0m amp_i \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_head(psi_output), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    398\u001b[0m )  \u001b[38;5;66;03m# (seq, batch, phys_dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/custom_transformer_layer.py:119\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    120\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/custom_transformer_layer.py:128\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/custom_modules.py:142\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     87\u001b[0m             query: Tensor,\n\u001b[1;32m     88\u001b[0m             key: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m             attn_mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m             average_attn_weights: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[Tensor]]:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mNote::\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m      head of shape :math:`(N, num_heads, L, S)`.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/transformer_quantum_state/model/custom_modules.py:175\u001b[0m, in \u001b[0;36mMultiheadAttention._forward_impl\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m    173\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_Q(query)\n\u001b[1;32m    174\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_K(key)\n\u001b[0;32m--> 175\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_V\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m q \u001b[38;5;241m=\u001b[39m scaling \u001b[38;5;241m*\u001b[39m q\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt.train(\n",
    "    epochs=30000,\n",
    "    monitor_dict=monitor_dict,\n",
    "    log_tensorboard=True,\n",
    "    prob_weight=10**6,\n",
    "    arg_weight=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
