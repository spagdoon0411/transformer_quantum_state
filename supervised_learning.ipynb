{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Supervised Optimizer\n",
    "\n",
    "From start to finish, on pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from Ising import Ising\n",
    "from model import TransformerModel\n",
    "from optimizer_supervised import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_setup():\n",
    "    # Setup for PyTorch:\n",
    "    if torch.cuda.is_available():\n",
    "        torch_device = torch.device(\"cuda\")\n",
    "        print(\"PyTorch is using GPU {}\".format(torch.cuda.current_device()))\n",
    "    else:\n",
    "        torch_device = torch.device(\"cpu\")\n",
    "        print(\"GPU unavailable; using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using GPU 0\n"
     ]
    }
   ],
   "source": [
    "gpu_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_tensor(tens, labels, opacity=0.7, size=5):\n",
    "\n",
    "    x = np.arange(tens.shape[0])\n",
    "    y = np.arange(tens.shape[1])\n",
    "    z = np.arange(tens.shape[2])\n",
    "\n",
    "    xlen = len(x)\n",
    "    ylen = len(y)\n",
    "    zlen = len(z)\n",
    "\n",
    "    print(f\"(x, y, z) = ({xlen}, {ylen}, {zlen})\")\n",
    "\n",
    "    X, Y, Z = np.meshgrid(x, y, z)\n",
    "\n",
    "    color_function = np.vectorize(lambda x, y, z: tens[x, y, z])\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Scatter3d(\n",
    "                x=X.flatten(),\n",
    "                y=Y.flatten(),\n",
    "                z=Z.flatten(),\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=size,\n",
    "                    # color=tens.swapaxes(1, 2)\n",
    "                    # .swapaxes(0, 2)\n",
    "                    # .swapaxes(1, 2)\n",
    "                    # .flatten(),  # set color to an array/list of desired values\n",
    "                    color=color_function(X, Y, Z).flatten(),\n",
    "                    colorscale=\"bupu\",  # choose a colorscale\n",
    "                    opacity=opacity,\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(xaxis_title=labels[0], yaxis_title=labels[1], zaxis_title=labels[2]),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Probabilistic Batched Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1718580740865/work/aten/src/ATen/native/TensorShape.cpp:3675.)\n",
      "  return func(*args, **kwargs)\n",
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes:\n",
      " tensor([[18]], device='cuda:0')\n",
      "Dimensions of parameter space: 1\n",
      "Number of units in a feedforward layer: 32\n"
     ]
    }
   ],
   "source": [
    "system_sizes = torch.arange(4, 4 + 2, 2).reshape(-1, 1)\n",
    "Hamiltonians = [Ising(size, periodic=True) for size in system_sizes]\n",
    "param_dim = Hamiltonians[0].param_dim\n",
    "embedding_size = 32\n",
    "n_head = 8\n",
    "n_hid = embedding_size\n",
    "n_layers = 8\n",
    "dropout = 0\n",
    "minibatch = 1000\n",
    "param_range = None\n",
    "point_of_interest = None\n",
    "use_SR = False\n",
    "\n",
    "print(\"Sizes:\\n\", system_sizes)\n",
    "print(\"Dimensions of parameter space:\", param_dim)\n",
    "print(\"Number of units in a feedforward layer:\", n_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "gaussian_coeff = 1 / math.sqrt(2 * math.pi)\n",
    "gaussian_mean = 1.0\n",
    "gaussian_std = 0.05\n",
    "probability_distribution = lambda param: gaussian_coeff * torch.exp(\n",
    "    -0.5 * (((param - gaussian_mean) ** 2) / gaussian_std**2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1718580740865/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for system size 18 from TFIM_ground_states/2024-07-24T19-26-39.836/18.arrow.\n",
      "(h_min, h_step, h_max) = (0.5, 0.01, 1.5).\n",
      "Hamiltonians: [<Ising.Ising object at 0x7d4f85601130>]\n"
     ]
    }
   ],
   "source": [
    "data_dir_path = os.path.join(\"TFIM_ground_states\", \"2024-07-24T19-26-39.836\")\n",
    "for ham in Hamiltonians:\n",
    "    ham.load_dataset(data_dir_path, batch_size=30000, samples_in_epoch=100)\n",
    "    ham.training_dataset.set_sampling_distribution(probability_distribution)\n",
    "\n",
    "print(\"Hamiltonians:\", Hamiltonians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>h</th>\n",
       "      <th>energy</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-19.143800</td>\n",
       "      <td>[0.6074848784174569, 0.07720456699543793, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-19.190860</td>\n",
       "      <td>[0.603442449533376, 0.07828052250174836, 0.078...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-19.238925</td>\n",
       "      <td>[0.5989958034035765, 0.07928533711807846, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-19.288001</td>\n",
       "      <td>[0.5951097360014603, 0.08034609824462283, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-19.338093</td>\n",
       "      <td>[0.5908188007555905, 0.08133439307972481, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18</td>\n",
       "      <td>1.46</td>\n",
       "      <td>-29.465884</td>\n",
       "      <td>[0.04926065985698775, 0.02149227513715912, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>18</td>\n",
       "      <td>1.47</td>\n",
       "      <td>-29.622582</td>\n",
       "      <td>[0.04807624290278999, 0.021117536779159472, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>18</td>\n",
       "      <td>1.48</td>\n",
       "      <td>-29.779636</td>\n",
       "      <td>[0.04693857781807465, 0.020755229985387716, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>18</td>\n",
       "      <td>1.49</td>\n",
       "      <td>-29.937037</td>\n",
       "      <td>[0.045845243723527766, 0.020404784899805916, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>18</td>\n",
       "      <td>1.50</td>\n",
       "      <td>-30.094778</td>\n",
       "      <td>[0.04479394902743367, 0.02006566175938715, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      N     h     energy                                              state\n",
       "0    18  0.50 -19.143800  [0.6074848784174569, 0.07720456699543793, 0.07...\n",
       "1    18  0.51 -19.190860  [0.603442449533376, 0.07828052250174836, 0.078...\n",
       "2    18  0.52 -19.238925  [0.5989958034035765, 0.07928533711807846, 0.07...\n",
       "3    18  0.53 -19.288001  [0.5951097360014603, 0.08034609824462283, 0.08...\n",
       "4    18  0.54 -19.338093  [0.5908188007555905, 0.08133439307972481, 0.08...\n",
       "..   ..   ...        ...                                                ...\n",
       "96   18  1.46 -29.465884  [0.04926065985698775, 0.02149227513715912, 0.0...\n",
       "97   18  1.47 -29.622582  [0.04807624290278999, 0.021117536779159472, 0....\n",
       "98   18  1.48 -29.779636  [0.04693857781807465, 0.020755229985387716, 0....\n",
       "99   18  1.49 -29.937037  [0.045845243723527766, 0.020404784899805916, 0...\n",
       "100  18  1.50 -30.094778  [0.04479394902743367, 0.02006566175938715, 0.0...\n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hamiltonians[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n",
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel = TransformerModel(\n",
    "    system_sizes,\n",
    "    param_dim,\n",
    "    embedding_size,\n",
    "    n_head,\n",
    "    n_hid,\n",
    "    n_layers,\n",
    "    dropout=dropout,\n",
    "    minibatch=minibatch,\n",
    ")\n",
    "\n",
    "results_dir = \"results\"\n",
    "paper_checkpoint_name = \"ckpt_100000_Ising_32_8_8_0.ckpt\"\n",
    "paper_checkpoint_path = os.path.join(results_dir, paper_checkpoint_name)\n",
    "checkpoint = torch.load(paper_checkpoint_path)\n",
    "testmodel.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): TQSPositionalEncoding1D(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_Q): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_K): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_V): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=6, out_features=32, bias=True)\n",
       "  (amp_head): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (phase_head): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer_supervised_batches import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(testmodel, Hamiltonians, point_of_interest=point_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x, y, z) = (101, 1, 262144)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_tensor(\n\u001b[1;32m      2\u001b[0m     Hamiltonians[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtraining_dataset\u001b[38;5;241m.\u001b[39msampled\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m      3\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     opacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      5\u001b[0m     size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36mplot_tensor\u001b[0;34m(tens, labels, opacity, size)\u001b[0m\n\u001b[1;32m     17\u001b[0m X, Y, Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(x, y, z)\n\u001b[1;32m     19\u001b[0m color_function \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m x, y, z: tens[x, y, z])\n\u001b[1;32m     21\u001b[0m fig \u001b[38;5;241m=\u001b[39m go\u001b[38;5;241m.\u001b[39mFigure(\n\u001b[1;32m     22\u001b[0m     data\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     23\u001b[0m         go\u001b[38;5;241m.\u001b[39mScatter3d(\n\u001b[1;32m     24\u001b[0m             x\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     25\u001b[0m             y\u001b[38;5;241m=\u001b[39mY\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     26\u001b[0m             z\u001b[38;5;241m=\u001b[39mZ\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     27\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m             marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     29\u001b[0m                 size\u001b[38;5;241m=\u001b[39msize,\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;66;03m# color=tens.swapaxes(1, 2)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;66;03m# .swapaxes(0, 2)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m                 \u001b[38;5;66;03m# .swapaxes(1, 2)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;66;03m# .flatten(),  # set color to an array/list of desired values\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m                 color\u001b[38;5;241m=\u001b[39mcolor_function(X, Y, Z)\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     35\u001b[0m                 colorscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbupu\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# choose a colorscale\u001b[39;00m\n\u001b[1;32m     36\u001b[0m                 opacity\u001b[38;5;241m=\u001b[39mopacity,\n\u001b[1;32m     37\u001b[0m             ),\n\u001b[1;32m     38\u001b[0m         )\n\u001b[1;32m     39\u001b[0m     ]\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(\n\u001b[1;32m     43\u001b[0m     scene\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(xaxis_title\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m], yaxis_title\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m1\u001b[39m], zaxis_title\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/numpy/lib/function_base.py:2372\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stage_2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_as_normal(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/numpy/lib/function_base.py:2365\u001b[0m, in \u001b[0;36mvectorize._call_as_normal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2363\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize_call(func\u001b[38;5;241m=\u001b[39mfunc, args\u001b[38;5;241m=\u001b[39mvargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/numpy/lib/function_base.py:2455\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m-> 2455\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ufunc(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2458\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mplot_tensor.<locals>.<lambda>\u001b[0;34m(x, y, z)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(x, y, z) = (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxlen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mylen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzlen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m X, Y, Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(x, y, z)\n\u001b[0;32m---> 19\u001b[0m color_function \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m x, y, z: tens[x, y, z])\n\u001b[1;32m     21\u001b[0m fig \u001b[38;5;241m=\u001b[39m go\u001b[38;5;241m.\u001b[39mFigure(\n\u001b[1;32m     22\u001b[0m     data\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     23\u001b[0m         go\u001b[38;5;241m.\u001b[39mScatter3d(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     ]\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(\n\u001b[1;32m     43\u001b[0m     scene\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(xaxis_title\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m], yaxis_title\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m1\u001b[39m], zaxis_title\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m     44\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_tensor(\n",
    "    Hamiltonians[0].training_dataset.sampled.unsqueeze(1).cpu().numpy(),\n",
    "    [\"batch\", \"system size\", \"parameter\"],\n",
    "    opacity=0.5,\n",
    "    size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of categories cannot exceed 2^24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, start_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised_batches.py:303\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, param_range, ensemble_id, start_iter)\u001b[0m\n\u001b[1;32m    300\u001b[0m system_size \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39msystem_size\n\u001b[1;32m    301\u001b[0m dataset \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mtraining_dataset\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m basis_states, params, psi_true \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_param(system_size\u001b[38;5;241m=\u001b[39msystem_size, param\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    307\u001b[0m     log_amp, log_phase \u001b[38;5;241m=\u001b[39m compute_psi(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    309\u001b[0m         basis_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,  \u001b[38;5;66;03m# NOTE: setting this puts compute_psi in to the new cross-J batch mode\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/tqs/batch_ising_dataset.py:265\u001b[0m, in \u001b[0;36mIsingRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples):\n\u001b[0;32m--> 265\u001b[0m         sampled_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_amp_probabilities,\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    268\u001b[0m             replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement,\n\u001b[1;32m    269\u001b[0m         )\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement:\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_amp_probabilities[sampled_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of categories cannot exceed 2^24"
     ]
    }
   ],
   "source": [
    "opt.train(epochs=1, start_iter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hamiltonians[0].training_dataset.sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x, y, z) = (101, 1, 16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           1,
           0,
           0,
           1,
           1,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           2,
           0,
           0,
           1,
           0,
           0,
           2,
           1,
           0,
           0,
           2,
           0,
           0,
           0,
           1,
           0,
           3,
           1,
           3,
           0,
           0,
           1,
           2,
           3,
           1,
           2,
           1,
           2,
           4,
           5,
           2,
           1,
           2,
           5,
           2,
           1,
           1,
           6,
           2,
           4,
           3,
           3,
           5,
           4,
           7,
           2,
           9,
           4,
           11,
           4,
           5,
           6,
           3,
           8,
           4,
           1,
           0,
           4,
           4,
           6,
           10,
           6,
           10,
           15,
           11,
           6,
           5,
           15,
           8,
           7,
           10,
           6,
           7,
           7,
           9,
           12,
           17,
           17,
           13,
           18,
           13,
           17,
           15,
           20,
           11,
           17,
           15,
           10,
           16,
           20,
           22,
           25,
           24,
           27,
           28,
           23,
           28,
           24,
           26,
           35,
           22,
           27,
           28,
           27,
           21,
           17,
           26,
           48,
           40,
           39,
           37,
           47,
           30,
           49,
           36,
           50,
           40,
           46,
           44,
           55,
           45,
           40,
           48,
           77,
           64,
           71,
           83,
           47,
           79,
           68,
           61,
           65,
           79,
           65,
           75,
           64,
           66,
           64,
           72,
           90,
           108,
           104,
           91,
           93,
           110,
           111,
           112,
           89,
           100,
           102,
           91,
           97,
           98,
           96,
           82,
           153,
           154,
           155,
           166,
           146,
           144,
           143,
           156,
           150,
           152,
           139,
           161,
           156,
           137,
           157,
           170,
           208,
           238,
           224,
           200,
           220,
           186,
           217,
           203,
           220,
           215,
           187,
           188,
           190,
           207,
           221,
           198,
           278,
           293,
           283,
           273,
           275,
           272,
           269,
           278,
           249,
           268,
           269,
           238,
           258,
           257,
           279,
           262,
           329,
           350,
           360,
           359,
           326,
           326,
           346,
           371,
           322,
           333,
           328,
           343,
           353,
           330,
           349,
           323,
           388,
           400,
           404,
           391,
           404,
           442,
           380,
           425,
           426,
           405,
           385,
           414,
           416,
           413,
           418,
           430,
           486,
           458,
           472,
           503,
           510,
           501,
           484,
           508,
           495,
           463,
           489,
           475,
           481,
           485,
           464,
           490,
           551,
           574,
           514,
           584,
           538,
           557,
           571,
           552,
           556,
           553,
           533,
           553,
           559,
           547,
           552,
           566,
           595,
           563,
           570,
           572,
           562,
           590,
           551,
           581,
           598,
           564,
           601,
           602,
           582,
           590,
           576,
           581,
           622,
           616,
           608,
           610,
           620,
           597,
           637,
           583,
           616,
           631,
           605,
           623,
           623,
           596,
           619,
           584,
           623,
           594,
           644,
           628,
           631,
           603,
           649,
           606,
           632,
           631,
           625,
           592,
           599,
           625,
           629,
           636,
           606,
           595,
           627,
           626,
           635,
           626,
           647,
           620,
           650,
           660,
           592,
           604,
           637,
           600,
           601,
           616,
           616,
           534,
           559,
           567,
           571,
           552,
           621,
           569,
           584,
           573,
           594,
           574,
           603,
           582,
           604,
           595,
           547,
           524,
           563,
           540,
           546,
           556,
           544,
           533,
           506,
           555,
           593,
           553,
           523,
           523,
           603,
           532,
           480,
           473,
           515,
           490,
           489,
           472,
           531,
           460,
           481,
           470,
           503,
           437,
           489,
           468,
           503,
           476,
           405,
           389,
           411,
           413,
           420,
           422,
           404,
           423,
           430,
           411,
           419,
           434,
           388,
           419,
           394,
           418,
           344,
           367,
           347,
           356,
           334,
           351,
           326,
           332,
           334,
           334,
           372,
           345,
           334,
           324,
           345,
           320,
           275,
           260,
           252,
           275,
           251,
           269,
           298,
           261,
           294,
           277,
           284,
           263,
           261,
           264,
           245,
           290,
           226,
           214,
           195,
           199,
           182,
           233,
           210,
           210,
           212,
           197,
           196,
           221,
           213,
           191,
           195,
           207,
           161,
           145,
           163,
           193,
           166,
           136,
           142,
           128,
           161,
           142,
           131,
           132,
           142,
           167,
           146,
           146,
           115,
           95,
           104,
           96,
           87,
           97,
           120,
           101,
           122,
           102,
           88,
           118,
           98,
           117,
           106,
           110,
           75,
           69,
           79,
           69,
           76,
           79,
           70,
           80,
           69,
           66,
           60,
           60,
           73,
           70,
           64,
           71,
           47,
           45,
           39,
           47,
           52,
           39,
           41,
           43,
           47,
           30,
           45,
           44,
           47,
           38,
           49,
           39,
           19,
           23,
           22,
           28,
           30,
           20,
           33,
           21,
           31,
           22,
           30,
           29,
           20,
           19,
           26,
           22,
           12,
           16,
           17,
           25,
           14,
           19,
           11,
           16,
           14,
           13,
           16,
           13,
           11,
           14,
           19,
           22,
           10,
           9,
           4,
           10,
           7,
           9,
           13,
           5,
           10,
           14,
           11,
           9,
           7,
           8,
           4,
           9,
           1,
           7,
           6,
           4,
           7,
           4,
           4,
           6,
           2,
           2,
           6,
           3,
           9,
           2,
           4,
           7,
           0,
           2,
           3,
           2,
           0,
           3,
           3,
           1,
           1,
           1,
           0,
           2,
           2,
           2,
           1,
           5,
           0,
           2,
           1,
           2,
           2,
           1,
           0,
           1,
           0,
           0,
           2,
           3,
           1,
           0,
           0,
           3,
           0,
           0,
           0,
           1,
           3,
           1,
           1,
           1,
           0,
           2,
           1,
           2,
           1,
           0,
           1,
           3,
           0,
           2,
           0,
           0,
           1,
           0,
           0,
           0,
           2,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          "colorscale": [
           [
            0,
            "rgb(247,252,253)"
           ],
           [
            0.125,
            "rgb(224,236,244)"
           ],
           [
            0.25,
            "rgb(191,211,230)"
           ],
           [
            0.375,
            "rgb(158,188,218)"
           ],
           [
            0.5,
            "rgb(140,150,198)"
           ],
           [
            0.625,
            "rgb(140,107,177)"
           ],
           [
            0.75,
            "rgb(136,65,157)"
           ],
           [
            0.875,
            "rgb(129,15,124)"
           ],
           [
            1,
            "rgb(77,0,75)"
           ]
          ],
          "opacity": 0.5,
          "size": 3
         },
         "mode": "markers",
         "type": "scatter3d",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "z": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15
         ]
        }
       ],
       "layout": {
        "scene": {
         "xaxis": {
          "title": {
           "text": "batch"
          }
         },
         "yaxis": {
          "title": {
           "text": "system size"
          }
         },
         "zaxis": {
          "title": {
           "text": "parameter"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tensor(\n",
    "    Hamiltonians[0].training_dataset.sampled.unsqueeze(1).cpu().numpy(),\n",
    "    [\"batch\", \"system size\", \"parameter\"],\n",
    "    opacity=0.5,\n",
    "    size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Non-Batched Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m point_of_interest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m use_SR \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m Hamiltonians \u001b[38;5;241m=\u001b[39m [Ising(L) \u001b[38;5;28;01mfor\u001b[39;00m L \u001b[38;5;129;01min\u001b[39;00m system_sizes]\n\u001b[1;32m     15\u001b[0m data_dir_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFIM_ground_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-07-24T19-26-39.836\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ham \u001b[38;5;129;01min\u001b[39;00m Hamiltonians:\n",
      "File \u001b[0;32m~/Projects/tqs/Ising.py:53\u001b[0m, in \u001b[0;36mIsing.__init__\u001b[0;34m(self, system_size, periodic)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_field \u001b[38;5;241m=\u001b[39m generate_spin_idx(\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_size\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexternal_field\u001b[39m\u001b[38;5;124m\"\u001b[39m, periodic\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m     ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZZ\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections),\n\u001b[1;32m     50\u001b[0m     ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_field),\n\u001b[1;32m     51\u001b[0m ]\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_basis()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# TODO: implement 2D symmetry\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/Ising.py:107\u001b[0m, in \u001b[0;36mIsing.get_basis\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m basis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn):\n\u001b[0;32m--> 107\u001b[0m     basis[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbinary_repr(i, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)])\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(basis\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/numpy/core/numeric.py:2027\u001b[0m, in \u001b[0;36mbinary_repr\u001b[0;34m(num, width)\u001b[0m\n\u001b[1;32m   2024\u001b[0m     binwidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(binary)\n\u001b[1;32m   2025\u001b[0m     outwidth \u001b[38;5;241m=\u001b[39m (binwidth \u001b[38;5;28;01mif\u001b[39;00m width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2026\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m builtins\u001b[38;5;241m.\u001b[39mmax(binwidth, width))\n\u001b[0;32m-> 2027\u001b[0m     warn_if_insufficient(width, binwidth)\n\u001b[1;32m   2028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary\u001b[38;5;241m.\u001b[39mzfill(outwidth)\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/numpy/core/numeric.py:2009\u001b[0m, in \u001b[0;36mbinary_repr.<locals>.warn_if_insufficient\u001b[0;34m(width, binwidth)\u001b[0m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwarn_if_insufficient\u001b[39m(width, binwidth):\n\u001b[0;32m-> 2009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m width \u001b[38;5;241m<\u001b[39m binwidth:\n\u001b[1;32m   2010\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2011\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsufficient bit width provided. This behavior \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2012\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill raise an error in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   2013\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "system_sizes = torch.arange(2, 16 + 1, 2).reshape(-1, 1)\n",
    "Hamiltonians = [Ising(size, periodic=True) for size in system_sizes]\n",
    "param_dim = Hamiltonians[0].param_dim\n",
    "embedding_size = 32\n",
    "n_head = 8\n",
    "n_hid = embedding_size\n",
    "n_layers = 8\n",
    "dropout = 0\n",
    "minibatch = 1000\n",
    "param_range = None\n",
    "point_of_interest = None\n",
    "use_SR = False\n",
    "\n",
    "Hamiltonians = [Ising(L) for L in system_sizes]\n",
    "data_dir_path = os.path.join(\"TFIM_ground_states\", \"2024-07-24T19-26-39.836\")\n",
    "for ham in Hamiltonians:\n",
    "    ham.load_dataset(data_dir_path)\n",
    "\n",
    "print(\"Sizes:\\n\", system_sizes)\n",
    "print(\"Hamiltonians:\", Hamiltonians)\n",
    "print(\"Dimensions of parameter space:\", param_dim)\n",
    "print(\"Number of units in a feedforward layer:\", n_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel = TransformerModel(\n",
    "    system_sizes,\n",
    "    param_dim,\n",
    "    embedding_size,\n",
    "    n_head,\n",
    "    n_hid,\n",
    "    n_layers,\n",
    "    dropout=dropout,\n",
    "    minibatch=minibatch,\n",
    ")\n",
    "\n",
    "results_dir = \"results\"\n",
    "paper_checkpoint_name = \"ckpt_100000_Ising_32_8_8_0.ckpt\"\n",
    "paper_checkpoint_path = os.path.join(results_dir, paper_checkpoint_name)\n",
    "checkpoint = torch.load(paper_checkpoint_path)\n",
    "testmodel.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): TQSPositionalEncoding1D(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_Q): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_K): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_V): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=6, out_features=32, bias=True)\n",
       "  (amp_head): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (phase_head): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(testmodel, Hamiltonians, point_of_interest=point_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n",
      "/home/spandan/Projects/tqs/model.py:228: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, start_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised_batches.py:304\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, param_range, ensemble_id, start_iter)\u001b[0m\n\u001b[1;32m    300\u001b[0m dataset \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mtraining_dataset\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m basis_states, params, psi_true \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_param(system_size\u001b[38;5;241m=\u001b[39msystem_size, param\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m     log_amp, log_phase \u001b[38;5;241m=\u001b[39m compute_psi(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    307\u001b[0m         basis_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,  \u001b[38;5;66;03m# NOTE: setting this puts compute_psi in to the new cross-J batch mode\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     )\n\u001b[1;32m    313\u001b[0m     psi_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpsi_from_logs(log_amp, log_phase)\n",
      "File \u001b[0;32m~/Projects/tqs/model.py:151\u001b[0m, in \u001b[0;36mTransformerModel.set_param\u001b[0;34m(self, system_size, param)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_range[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dim) \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_range[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam \u001b[38;5;241m=\u001b[39m param\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "param_range = torch.tensor([[0.5, 1.5]])\n",
    "param_step = torch.tensor([0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel.minibatch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([16], device='cuda:0') spins at point (0.5,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/Projects/tqs/model.py:228: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# with cProfile.Profile() as pr:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# with torch.autograd.profiler.profile(use_cuda=True) as prof:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(epochs\u001b[38;5;241m=\u001b[39mepochs, param_range\u001b[38;5;241m=\u001b[39mparam_range, param_step\u001b[38;5;241m=\u001b[39mparam_step, start_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised.py:315\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, param_range, param_step, ensemble_id, start_iter)\u001b[0m\n\u001b[1;32m    309\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# NOTE: the system size is constant for this inner loop but must be reset\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# here because leaving it out (or, equivalently, setting it to None) would\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# cause the model to sample a random system size. See set_param in model.py.\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m dummy_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(H\u001b[38;5;241m.\u001b[39mbasis, compute_phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRan forward for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m spins at point \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# loss = self.calculate_mse_step(\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m#     H, param, basis_batch=None, use_symmetry=True\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/model.py:309\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, spins, compute_phase)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# src_i = src_i + self.pos_embedding[:len(src_i)]  # (seq, batch, embedding)\u001b[39;00m\n\u001b[1;32m    308\u001b[0m src_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src_i, system_size)  \u001b[38;5;66;03m# (seq, batch, embedding)\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m output_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(\n\u001b[1;32m    310\u001b[0m     src_i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_mask\n\u001b[1;32m    311\u001b[0m )  \u001b[38;5;66;03m# (seq, batch, embedding)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m psi_output \u001b[38;5;241m=\u001b[39m output_i[\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_prefix_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m :\n\u001b[1;32m    314\u001b[0m ]  \u001b[38;5;66;03m# only use the physical degrees of freedom\u001b[39;00m\n\u001b[1;32m    315\u001b[0m amp_i \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_head(psi_output), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    317\u001b[0m )  \u001b[38;5;66;03m# (seq, batch, phys_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, src_mask\u001b[38;5;241m=\u001b[39mmask, is_causal\u001b[38;5;241m=\u001b[39mis_causal, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/custom_transformer_layer.py:119\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    120\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Projects/tqs/custom_transformer_layer.py:128\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    129\u001b[0m         x,\n\u001b[1;32m    130\u001b[0m         x,\n\u001b[1;32m    131\u001b[0m         x,\n\u001b[1;32m    132\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    133\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    134\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    135\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/custom_modules.py:142\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     87\u001b[0m             query: Tensor,\n\u001b[1;32m     88\u001b[0m             key: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m             attn_mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m             average_attn_weights: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[Tensor]]:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mNote::\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m      head of shape :math:`(N, num_heads, L, S)`.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(query, key, value, key_padding_mask,\n\u001b[1;32m    143\u001b[0m                               need_weights, attn_mask, average_attn_weights)\n",
      "File \u001b[0;32m~/Projects/tqs/custom_modules.py:259\u001b[0m, in \u001b[0;36mMultiheadAttention._forward_impl\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m         key_padding_mask \u001b[38;5;241m=\u001b[39m nnF\u001b[38;5;241m.\u001b[39mpad(key_padding_mask, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 259\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(attn_output_weights\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m [bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len]\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# with cProfile.Profile() as pr:\n",
    "# with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "opt.train(epochs=epochs, param_range=param_range, param_step=param_step, start_iter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
