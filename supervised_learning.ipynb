{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Supervised Optimizer\n",
    "\n",
    "From start to finish, on pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from Ising import Ising\n",
    "from model import TransformerModel\n",
    "from optimizer_supervised import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_setup():\n",
    "    # Setup for PyTorch:\n",
    "    if torch.cuda.is_available():\n",
    "        torch_device = torch.device(\"cuda\")\n",
    "        print(\"PyTorch is using GPU {}\".format(torch.cuda.current_device()))\n",
    "    else:\n",
    "        torch_device = torch.device(\"cpu\")\n",
    "        print(\"GPU unavailable; using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  1 09:17:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0  On |                  N/A |\n",
      "| 64%   52C    P2             124W / 350W |   1693MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2501      G   /usr/lib/xorg/Xorg                           39MiB |\n",
      "|    0   N/A  N/A      4481      G   /usr/lib/xorg/Xorg                          281MiB |\n",
      "|    0   N/A  N/A      4717    C+G   ...libexec/gnome-remote-desktop-daemon      258MiB |\n",
      "|    0   N/A  N/A      4751      G   /usr/bin/gnome-shell                         72MiB |\n",
      "|    0   N/A  N/A      5622      G   ...erProcess --variations-seed-version      261MiB |\n",
      "|    0   N/A  N/A     15831      G   ...seed-version=20240731-180222.819000       40MiB |\n",
      "|    0   N/A  N/A     36049      C   ...ndan/anaconda3/envs/tqs2/bin/python      344MiB |\n",
      "|    0   N/A  N/A    161664      C   ...ndan/anaconda3/envs/tqs2/bin/python      344MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using GPU 0\n"
     ]
    }
   ],
   "source": [
    "gpu_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_default_device(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_tensor(tens, labels, opacity=0.7, size=5):\n",
    "\n",
    "    x = np.arange(tens.shape[0])\n",
    "    y = np.arange(tens.shape[1])\n",
    "    z = np.arange(tens.shape[2])\n",
    "\n",
    "    xlen = len(x)\n",
    "    ylen = len(y)\n",
    "    zlen = len(z)\n",
    "\n",
    "    print(f\"(x, y, z) = ({xlen}, {ylen}, {zlen})\")\n",
    "\n",
    "    X, Y, Z = np.meshgrid(x, y, z)\n",
    "\n",
    "    color_function = np.vectorize(lambda x, y, z: tens[x, y, z])\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Scatter3d(\n",
    "                x=X.flatten(),\n",
    "                y=Y.flatten(),\n",
    "                z=Z.flatten(),\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=size,\n",
    "                    # color=tens.swapaxes(1, 2)\n",
    "                    # .swapaxes(0, 2)\n",
    "                    # .swapaxes(1, 2)\n",
    "                    # .flatten(),  # set color to an array/list of desired values\n",
    "                    color=color_function(X, Y, Z).flatten(),\n",
    "                    colorscale=\"bupu\",  # choose a colorscale\n",
    "                    opacity=opacity,\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(xaxis_title=labels[0], yaxis_title=labels[1], zaxis_title=labels[2]),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Probabilistic Batched Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes:\n",
      " tensor([[15]], device='cuda:0')\n",
      "Dimensions of parameter space: 1\n",
      "Number of units in a feedforward layer: 32\n"
     ]
    }
   ],
   "source": [
    "system_sizes = torch.arange(15, 15 + 2, 2).reshape(-1, 1)\n",
    "Hamiltonians = [Ising(size, periodic=True, get_basis=True) for size in system_sizes]\n",
    "param_dim = Hamiltonians[0].param_dim\n",
    "embedding_size = 32\n",
    "n_head = 8\n",
    "n_hid = embedding_size\n",
    "n_layers = 8\n",
    "dropout = 0\n",
    "minibatch = 10000\n",
    "param_range = None\n",
    "point_of_interest = None\n",
    "use_SR = False\n",
    "\n",
    "print(\"Sizes:\\n\", system_sizes)\n",
    "print(\"Dimensions of parameter space:\", param_dim)\n",
    "print(\"Number of units in a feedforward layer:\", n_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "gaussian_coeff = 1 / math.sqrt(2 * math.pi)\n",
    "gaussian_mean = 1.0\n",
    "gaussian_std = 0.05\n",
    "probability_distribution = lambda param: gaussian_coeff * torch.exp(\n",
    "    -0.5 * (((param - gaussian_mean) ** 2) / gaussian_std**2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for system size 15 from TFIM_ground_states/2024-07-24T19-26-39.836/15.arrow.\n",
      "(h_min, h_step, h_max) = (0.5, 0.01, 1.5).\n",
      "Hamiltonians: [<Ising.Ising object at 0x74d301df1190>]\n"
     ]
    }
   ],
   "source": [
    "data_dir_path = os.path.join(\"TFIM_ground_states\", \"2024-07-24T19-26-39.836\")\n",
    "# data_dir_path = os.path.join(\"TFIM_ground_states\", \"2024-07-24T16-19-00.994\")\n",
    "\n",
    "for ham in Hamiltonians:\n",
    "    ham.load_dataset(\n",
    "        data_dir_path,\n",
    "        batch_size=30000,\n",
    "        samples_in_epoch=100,\n",
    "        sampling_type=\"sequential\",\n",
    "    )\n",
    "    # ham.training_dataset.set_sampling_distribution(probability_distribution)\n",
    "\n",
    "print(\"Hamiltonians:\", Hamiltonians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>h</th>\n",
       "      <th>energy</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-15.953170</td>\n",
       "      <td>[0.6230531142436205, 0.07918341825738834, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-15.992388</td>\n",
       "      <td>[0.6195952254116485, 0.0803763042893551, 0.080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-16.032444</td>\n",
       "      <td>[0.6160625240927698, 0.08154485611438121, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-16.073342</td>\n",
       "      <td>[0.6152313284867159, 0.08306336670116583, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-16.115088</td>\n",
       "      <td>[0.6087701876950788, 0.08380647181873989, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15</td>\n",
       "      <td>1.46</td>\n",
       "      <td>-24.555332</td>\n",
       "      <td>[0.08129764151807829, 0.03547150446859837, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>15</td>\n",
       "      <td>1.47</td>\n",
       "      <td>-24.685879</td>\n",
       "      <td>[0.07967117860998968, 0.034997069483799985, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>15</td>\n",
       "      <td>1.48</td>\n",
       "      <td>-24.816726</td>\n",
       "      <td>[0.07810205373098152, 0.03453632521710248, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>15</td>\n",
       "      <td>1.49</td>\n",
       "      <td>-24.947865</td>\n",
       "      <td>[0.07658757690991246, 0.03408871756724923, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>15</td>\n",
       "      <td>1.50</td>\n",
       "      <td>-25.079288</td>\n",
       "      <td>[0.07512521201143109, 0.03365371918208109, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      N     h     energy                                              state\n",
       "0    15  0.50 -15.953170  [0.6230531142436205, 0.07918341825738834, 0.07...\n",
       "1    15  0.51 -15.992388  [0.6195952254116485, 0.0803763042893551, 0.080...\n",
       "2    15  0.52 -16.032444  [0.6160625240927698, 0.08154485611438121, 0.08...\n",
       "3    15  0.53 -16.073342  [0.6152313284867159, 0.08306336670116583, 0.08...\n",
       "4    15  0.54 -16.115088  [0.6087701876950788, 0.08380647181873989, 0.08...\n",
       "..   ..   ...        ...                                                ...\n",
       "96   15  1.46 -24.555332  [0.08129764151807829, 0.03547150446859837, 0.0...\n",
       "97   15  1.47 -24.685879  [0.07967117860998968, 0.034997069483799985, 0....\n",
       "98   15  1.48 -24.816726  [0.07810205373098152, 0.03453632521710248, 0.0...\n",
       "99   15  1.49 -24.947865  [0.07658757690991246, 0.03408871756724923, 0.0...\n",
       "100  15  1.50 -25.079288  [0.07512521201143109, 0.03365371918208109, 0.0...\n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hamiltonians[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n",
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel = TransformerModel(\n",
    "    system_sizes,\n",
    "    param_dim,\n",
    "    embedding_size,\n",
    "    n_head,\n",
    "    n_hid,\n",
    "    n_layers,\n",
    "    dropout=dropout,\n",
    "    minibatch=minibatch,\n",
    ")\n",
    "\n",
    "results_dir = \"results\"\n",
    "paper_checkpoint_name = \"ckpt_100000_Ising_32_8_8_0.ckpt\"\n",
    "paper_checkpoint_path = os.path.join(results_dir, paper_checkpoint_name)\n",
    "checkpoint = torch.load(paper_checkpoint_path)\n",
    "testmodel.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): TQSPositionalEncoding1D(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_Q): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_K): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_V): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=6, out_features=32, bias=True)\n",
       "  (amp_head): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (phase_head): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer_supervised_batches import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(testmodel, Hamiltonians, point_of_interest=point_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-39.        , -39.00420011, -39.01680176, -39.03780892,\n",
       "       -39.06722821, -39.10506893, -39.1513431 , -39.20606547,\n",
       "       -39.26925361, -39.3409279 , -39.42111169, -39.5098313 ,\n",
       "       -39.60711613, -39.7129988 , -39.82751523, -39.95070478,\n",
       "       -40.0826104 , -40.2232788 , -40.37276064, -40.53111075,\n",
       "       -40.69838833, -40.87465727, -41.05998641, -41.25444989,\n",
       "       -41.45812755, -41.67110535, -41.89347585, -42.12533881,\n",
       "       -42.36680181, -42.61798101, -42.879002  , -43.15000084,\n",
       "       -43.43112524, -43.72253594, -44.02440843, -44.33693501,\n",
       "       -44.66032733, -44.99481972, -45.34067359, -45.69818378,\n",
       "       -46.06768807, -46.44958242, -46.84434575, -47.25257988,\n",
       "       -47.67506782, -48.11283922, -48.56718438, -49.0394836 ,\n",
       "       -49.53074211, -50.04104421, -50.56943379, -51.11432399,\n",
       "       -51.67399663, -52.24687977, -52.83162458, -53.4270924 ,\n",
       "       -54.03231875, -54.64647861, -55.2688584 , -55.8988346 ,\n",
       "       -56.53585745, -57.1794384 , -57.82914044, -58.48457031,\n",
       "       -59.14537242, -59.8112237 , -60.48182955, -61.15692033,\n",
       "       -61.83624849, -62.51958609, -63.20672267, -63.89746351,\n",
       "       -64.59162797, -65.28904819, -65.98956787, -66.69304118,\n",
       "       -67.39933191, -68.10831256, -68.81986363, -69.53387301,\n",
       "       -70.2502353 , -70.96885136, -71.6896278 , -72.41247654,\n",
       "       -73.13731441, -73.86406285, -74.5926475 , -75.32299799,\n",
       "       -76.05504762, -76.7887331 , -77.52399439, -78.2607744 ,\n",
       "       -78.9990189 , -79.73867624, -80.47969729, -81.22203519,\n",
       "       -81.96564528, -82.71048496, -83.45651355, -84.20369218,\n",
       "       -84.95198371])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drmg40path = os.path.join(\"results\", \"E_dmrg_40.npy\")\n",
    "dmrg40 = np.load(drmg40path)\n",
    "dmrg40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ising40 = Ising(\n",
    "    torch.tensor([40]),\n",
    "    periodic=False,\n",
    "    get_basis=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/tenpy/models/model.py:1946: UserWarning: Invalid type for key \"L\". Expected <class 'int'>. Got Tensor.\n",
      "  L = model_params.get('L', 2, int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-39.00420011001162,\n",
       " <tenpy.networks.mps.MPS at 0x74d387e52cf0>,\n",
       " <tenpy.models.spins.SpinModel at 0x74d2b0ab5f70>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ising40.DMRG(param=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 ,\n",
       "       0.22, 0.24, 0.26, 0.28, 0.3 , 0.32, 0.34, 0.36, 0.38, 0.4 , 0.42,\n",
       "       0.44, 0.46, 0.48, 0.5 , 0.52, 0.54, 0.56, 0.58, 0.6 , 0.62, 0.64,\n",
       "       0.66, 0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86,\n",
       "       0.88, 0.9 , 0.92, 0.94, 0.96, 0.98, 1.  , 1.02, 1.04, 1.06, 1.08,\n",
       "       1.1 , 1.12, 1.14, 1.16, 1.18, 1.2 , 1.22, 1.24, 1.26, 1.28, 1.3 ,\n",
       "       1.32, 1.34, 1.36, 1.38, 1.4 , 1.42, 1.44, 1.46, 1.48, 1.5 , 1.52,\n",
       "       1.54, 1.56, 1.58, 1.6 , 1.62, 1.64, 1.66, 1.68, 1.7 , 1.72, 1.74,\n",
       "       1.76, 1.78, 1.8 , 1.82, 1.84, 1.86, 1.88, 1.9 , 1.92, 1.94, 1.96,\n",
       "       1.98, 2.  ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmrg40_h_values = np.linspace(0, 2, 101)\n",
    "dmrg40_h_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(len(dmrg40_h_values))\n",
    "print(len(dmrg40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([40], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ising40.system_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneidx = np.where(dmrg40_h_values == 1.0)[0][0]\n",
    "oneidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-50.56943379479509"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmrg40[oneidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iter 0 - Loss for system size tensor([15], device='cuda:0') and h-range 0.5-0.5: 0.11009185016155243\n",
      "compute_observable time: 0.0005548000335693359\n",
      "compute_observable time: 3.0858829021453857\n",
      "Sample time: 0.4039294719696045, Eloc time: 3.086578845977783\n",
      "\tparam=tensor([1.], device='cuda:0'), system_size=tensor([40], device='cuda:0') - Relative Error: tensor([[51.5510]], device='cuda:0', dtype=torch.float64)\n",
      "\t\tEnergy: (-50.55104446411133+0.0024252498988062143j), Variance: 8.536142559023574e-06, Real: -1.2637760639190674, Imag: 6.0631246014963835e-05\n",
      "Time breakdown: psi: 0.0041866302490234375, loss: 0.00017142295837402344, backprop: 0.7667841911315918, energy: 3.491574764251709\n",
      "Epoch 0 iter 1 - Loss for system size tensor([15], device='cuda:0') and h-range 0.5-0.51: 0.11004701256752014\n",
      "compute_observable time: 0.0005495548248291016\n",
      "compute_observable time: 2.995652437210083\n",
      "Sample time: 0.39482808113098145, Eloc time: 2.99642014503479\n",
      "\tparam=tensor([1.], device='cuda:0'), system_size=tensor([40], device='cuda:0') - Relative Error: tensor([[51.5474]], device='cuda:0', dtype=torch.float64)\n",
      "\t\tEnergy: (-50.54741668701172-0.0017338197212666273j), Variance: 7.839292265998665e-06, Real: -1.2636854648590088, Imag: -4.334549157647416e-05\n",
      "Time breakdown: psi: 0.003697633743286133, loss: 0.0002524852752685547, backprop: 0.7530074119567871, energy: 3.3925023078918457\n",
      "Epoch 0 iter 2 - Loss for system size tensor([15], device='cuda:0') and h-range 0.51-0.52: 0.1097417026758194\n",
      "compute_observable time: 0.0005178451538085938\n",
      "compute_observable time: 2.9650096893310547\n",
      "Sample time: 0.38991522789001465, Eloc time: 2.9656500816345215\n",
      "\tparam=tensor([1.], device='cuda:0'), system_size=tensor([40], device='cuda:0') - Relative Error: tensor([[51.5579]], device='cuda:0', dtype=torch.float64)\n",
      "\t\tEnergy: (-50.557891845703125+0.0022648342419415712j), Variance: 7.21045580576174e-06, Real: -1.2639472484588623, Imag: 5.662085459334776e-05\n",
      "Time breakdown: psi: 0.0038230419158935547, loss: 0.00024247169494628906, backprop: 0.7584583759307861, energy: 3.356623888015747\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      2\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      3\u001b[0m     start_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m     monitor_params\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1.0\u001b[39m]]),\n\u001b[1;32m      5\u001b[0m     monitor_hamiltonians\u001b[38;5;241m=\u001b[39m[ising40],\n\u001b[1;32m      6\u001b[0m     monitor_energies\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([[dmrg40_h_values[oneidx]]])\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised_batches.py:429\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, monitor_params, monitor_hamiltonians, monitor_energies, param_range, ensemble_id, start_iter)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    428\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    430\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    431\u001b[0m backprop_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     adam(\n\u001b[1;32m    169\u001b[0m         params_with_grad,\n\u001b[1;32m    170\u001b[0m         grads,\n\u001b[1;32m    171\u001b[0m         exp_avgs,\n\u001b[1;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    174\u001b[0m         state_steps,\n\u001b[1;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m func(params,\n\u001b[1;32m    319\u001b[0m      grads,\n\u001b[1;32m    320\u001b[0m      exp_avgs,\n\u001b[1;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    323\u001b[0m      state_steps,\n\u001b[1;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/optim/adam.py:567\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    565\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    568\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    570\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt.train(\n",
    "    epochs=1,\n",
    "    start_iter=0,\n",
    "    monitor_params=torch.tensor([[1.0]]),\n",
    "    monitor_hamiltonians=[ising40],\n",
    "    monitor_energies=torch.tensor([[dmrg40_h_values[oneidx]]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hamiltonians[0].training_dataset.sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x, y, z) = (101, 1, 16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           1,
           0,
           0,
           1,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           4,
           0,
           0,
           0,
           1,
           2,
           1,
           0,
           2,
           0,
           1,
           4,
           1,
           2,
           0,
           1,
           0,
           4,
           4,
           2,
           3,
           2,
           1,
           3,
           2,
           3,
           2,
           3,
           5,
           4,
           8,
           3,
           4,
           3,
           3,
           5,
           7,
           3,
           5,
           2,
           2,
           3,
           4,
           9,
           8,
           13,
           11,
           6,
           3,
           8,
           9,
           11,
           6,
           9,
           7,
           14,
           9,
           9,
           11,
           21,
           23,
           19,
           22,
           22,
           12,
           23,
           20,
           20,
           17,
           24,
           22,
           23,
           26,
           24,
           23,
           42,
           34,
           32,
           35,
           37,
           45,
           39,
           33,
           28,
           27,
           39,
           48,
           37,
           31,
           30,
           38,
           58,
           63,
           58,
           56,
           69,
           60,
           54,
           67,
           49,
           56,
           66,
           64,
           57,
           51,
           62,
           49,
           78,
           73,
           80,
           80,
           83,
           82,
           82,
           77,
           82,
           88,
           79,
           83,
           79,
           86,
           88,
           81,
           98,
           95,
           97,
           96,
           97,
           96,
           98,
           97,
           99,
           95,
           97,
           93,
           98,
           98,
           97,
           95,
           100,
           99,
           99,
           100,
           100,
           99,
           97,
           100,
           98,
           100,
           100,
           99,
           100,
           100,
           100,
           97,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           99,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           99,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           100,
           99,
           100,
           100,
           100,
           100,
           99,
           99,
           99,
           98,
           100,
           100,
           93,
           93,
           96,
           98,
           94,
           95,
           92,
           93,
           93,
           93,
           94,
           96,
           97,
           99,
           94,
           96,
           78,
           89,
           83,
           85,
           82,
           80,
           78,
           78,
           81,
           74,
           85,
           83,
           78,
           79,
           84,
           82,
           50,
           63,
           65,
           64,
           61,
           58,
           67,
           57,
           60,
           60,
           55,
           60,
           60,
           61,
           60,
           61,
           36,
           46,
           41,
           44,
           28,
           37,
           35,
           40,
           38,
           42,
           38,
           33,
           25,
           36,
           32,
           32,
           28,
           21,
           20,
           18,
           19,
           30,
           19,
           20,
           20,
           19,
           18,
           27,
           22,
           23,
           21,
           22,
           11,
           13,
           6,
           14,
           7,
           12,
           10,
           7,
           9,
           9,
           9,
           11,
           10,
           8,
           10,
           11,
           8,
           2,
           2,
           5,
           6,
           4,
           5,
           6,
           9,
           4,
           3,
           4,
           7,
           8,
           6,
           3,
           3,
           4,
           2,
           5,
           2,
           2,
           1,
           3,
           2,
           1,
           2,
           3,
           4,
           7,
           1,
           4,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           0,
           2,
           0,
           0,
           0,
           1,
           0,
           2,
           4,
           0,
           0,
           0,
           1,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          "colorscale": [
           [
            0,
            "rgb(247,252,253)"
           ],
           [
            0.125,
            "rgb(224,236,244)"
           ],
           [
            0.25,
            "rgb(191,211,230)"
           ],
           [
            0.375,
            "rgb(158,188,218)"
           ],
           [
            0.5,
            "rgb(140,150,198)"
           ],
           [
            0.625,
            "rgb(140,107,177)"
           ],
           [
            0.75,
            "rgb(136,65,157)"
           ],
           [
            0.875,
            "rgb(129,15,124)"
           ],
           [
            1,
            "rgb(77,0,75)"
           ]
          ],
          "opacity": 0.5,
          "size": 3
         },
         "mode": "markers",
         "type": "scatter3d",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          61,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          66,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          67,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          68,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          71,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          73,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          76,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          77,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          78,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          79,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          80,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          81,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          82,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          83,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          85,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          86,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          87,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          88,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          90,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          91,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          92,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          93,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          94,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          96,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          97,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          98,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          99,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "z": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15
         ]
        }
       ],
       "layout": {
        "scene": {
         "xaxis": {
          "title": {
           "text": "batch"
          }
         },
         "yaxis": {
          "title": {
           "text": "system size"
          }
         },
         "zaxis": {
          "title": {
           "text": "parameter"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tensor(\n",
    "    Hamiltonians[0].training_dataset.sampled.unsqueeze(1).cpu().numpy(),\n",
    "    [\"batch\", \"system size\", \"parameter\"],\n",
    "    opacity=0.5,\n",
    "    size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Non-Batched Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m point_of_interest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m use_SR \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m Hamiltonians \u001b[38;5;241m=\u001b[39m [Ising(L) \u001b[38;5;28;01mfor\u001b[39;00m L \u001b[38;5;129;01min\u001b[39;00m system_sizes]\n\u001b[1;32m     15\u001b[0m data_dir_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFIM_ground_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-07-24T19-26-39.836\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ham \u001b[38;5;129;01min\u001b[39;00m Hamiltonians:\n",
      "File \u001b[0;32m~/Projects/tqs/Ising.py:53\u001b[0m, in \u001b[0;36mIsing.__init__\u001b[0;34m(self, system_size, periodic)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_field \u001b[38;5;241m=\u001b[39m generate_spin_idx(\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_size\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexternal_field\u001b[39m\u001b[38;5;124m\"\u001b[39m, periodic\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m     ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZZ\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections),\n\u001b[1;32m     50\u001b[0m     ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_field),\n\u001b[1;32m     51\u001b[0m ]\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_basis()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# TODO: implement 2D symmetry\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/Ising.py:107\u001b[0m, in \u001b[0;36mIsing.get_basis\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m basis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn):\n\u001b[0;32m--> 107\u001b[0m     basis[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbinary_repr(i, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)])\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(basis\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/numpy/core/numeric.py:2027\u001b[0m, in \u001b[0;36mbinary_repr\u001b[0;34m(num, width)\u001b[0m\n\u001b[1;32m   2024\u001b[0m     binwidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(binary)\n\u001b[1;32m   2025\u001b[0m     outwidth \u001b[38;5;241m=\u001b[39m (binwidth \u001b[38;5;28;01mif\u001b[39;00m width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2026\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m builtins\u001b[38;5;241m.\u001b[39mmax(binwidth, width))\n\u001b[0;32m-> 2027\u001b[0m     warn_if_insufficient(width, binwidth)\n\u001b[1;32m   2028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary\u001b[38;5;241m.\u001b[39mzfill(outwidth)\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/numpy/core/numeric.py:2009\u001b[0m, in \u001b[0;36mbinary_repr.<locals>.warn_if_insufficient\u001b[0;34m(width, binwidth)\u001b[0m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwarn_if_insufficient\u001b[39m(width, binwidth):\n\u001b[0;32m-> 2009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m width \u001b[38;5;241m<\u001b[39m binwidth:\n\u001b[1;32m   2010\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2011\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsufficient bit width provided. This behavior \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2012\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill raise an error in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   2013\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "system_sizes = torch.arange(2, 16 + 1, 2).reshape(-1, 1)\n",
    "Hamiltonians = [Ising(size, periodic=True) for size in system_sizes]\n",
    "param_dim = Hamiltonians[0].param_dim\n",
    "embedding_size = 32\n",
    "n_head = 8\n",
    "n_hid = embedding_size\n",
    "n_layers = 8\n",
    "dropout = 0\n",
    "minibatch = 1000\n",
    "param_range = None\n",
    "point_of_interest = None\n",
    "use_SR = False\n",
    "\n",
    "Hamiltonians = [Ising(L) for L in system_sizes]\n",
    "data_dir_path = os.path.join(\"TFIM_ground_states\", \"2024-07-24T19-26-39.836\")\n",
    "for ham in Hamiltonians:\n",
    "    ham.load_dataset(data_dir_path)\n",
    "\n",
    "print(\"Sizes:\\n\", system_sizes)\n",
    "print(\"Hamiltonians:\", Hamiltonians)\n",
    "print(\"Dimensions of parameter space:\", param_dim)\n",
    "print(\"Number of units in a feedforward layer:\", n_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel = TransformerModel(\n",
    "    system_sizes,\n",
    "    param_dim,\n",
    "    embedding_size,\n",
    "    n_head,\n",
    "    n_hid,\n",
    "    n_layers,\n",
    "    dropout=dropout,\n",
    "    minibatch=minibatch,\n",
    ")\n",
    "\n",
    "results_dir = \"results\"\n",
    "paper_checkpoint_name = \"ckpt_100000_Ising_32_8_8_0.ckpt\"\n",
    "paper_checkpoint_path = os.path.join(results_dir, paper_checkpoint_name)\n",
    "checkpoint = torch.load(paper_checkpoint_path)\n",
    "testmodel.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): TQSPositionalEncoding1D(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_Q): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_K): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (linear_V): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=6, out_features=32, bias=True)\n",
       "  (amp_head): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (phase_head): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(testmodel, Hamiltonians, point_of_interest=point_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n",
      "/home/spandan/Projects/tqs/model.py:228: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, start_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised_batches.py:304\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, param_range, ensemble_id, start_iter)\u001b[0m\n\u001b[1;32m    300\u001b[0m dataset \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mtraining_dataset\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m basis_states, params, psi_true \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_param(system_size\u001b[38;5;241m=\u001b[39msystem_size, param\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m     log_amp, log_phase \u001b[38;5;241m=\u001b[39m compute_psi(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    307\u001b[0m         basis_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,  \u001b[38;5;66;03m# NOTE: setting this puts compute_psi in to the new cross-J batch mode\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     )\n\u001b[1;32m    313\u001b[0m     psi_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpsi_from_logs(log_amp, log_phase)\n",
      "File \u001b[0;32m~/Projects/tqs/model.py:151\u001b[0m, in \u001b[0;36mTransformerModel.set_param\u001b[0;34m(self, system_size, param)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_range[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dim) \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_range[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam \u001b[38;5;241m=\u001b[39m param\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "param_range = torch.tensor([[0.5, 1.5]])\n",
    "param_step = torch.tensor([0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel.minibatch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([2], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([4], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([6], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([8], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([10], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([12], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5099999997764826,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5199999995529652,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5299999993294477,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5399999991059303,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5499999988824129,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5599999986588955,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5699999984353781,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5799999982118607,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5899999979883432,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.5999999977648258,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6099999975413084,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.619999997317791,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6299999970942736,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6399999968707561,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6499999966472387,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6599999964237213,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6699999962002039,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6799999959766865,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6899999957531691,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.6999999955296516,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7099999953061342,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7199999950826168,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7299999948590994,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.739999994635582,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7499999944120646,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7599999941885471,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7699999939650297,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7799999937415123,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7899999935179949,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.7999999932944775,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.80999999307096,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8199999928474426,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8299999926239252,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8399999924004078,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8499999921768904,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.859999991953373,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8699999917298555,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8799999915063381,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8899999912828207,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.8999999910593033,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9099999908357859,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9199999906122684,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.929999990388751,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9399999901652336,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9499999899417162,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9599999897181988,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9699999894946814,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9799999892711639,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9899999890476465,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (0.9999999888241291,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0099999886006117,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0199999883770943,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0299999881535769,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0399999879300594,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.049999987706542,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0599999874830246,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0699999872595072,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0799999870359898,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.0899999868124723,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.099999986588955,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1099999863654375,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.11999998614192,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1299999859184027,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1399999856948853,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1499999854713678,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1599999852478504,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.169999985024333,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1799999848008156,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1899999845772982,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.1999999843537807,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2099999841302633,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.219999983906746,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2299999836832285,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.239999983459711,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2499999832361937,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2599999830126762,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2699999827891588,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2799999825656414,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.289999982342124,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.2999999821186066,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3099999818950891,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3199999816715717,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3299999814480543,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.339999981224537,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3499999810010195,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.359999980777502,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3699999805539846,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3799999803304672,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3899999801069498,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.3999999798834324,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.409999979659915,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4199999794363976,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4299999792128801,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4399999789893627,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4499999787658453,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4599999785423279,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4699999783188105,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.479999978095293,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4899999778717756,)\n",
      "Ran forward for tensor([14], device='cuda:0') spins at point (1.4999999776482582,)\n",
      "Ran forward for tensor([16], device='cuda:0') spins at point (0.5,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/Projects/tqs/model.py:228: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# with cProfile.Profile() as pr:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# with torch.autograd.profiler.profile(use_cuda=True) as prof:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(epochs\u001b[38;5;241m=\u001b[39mepochs, param_range\u001b[38;5;241m=\u001b[39mparam_range, param_step\u001b[38;5;241m=\u001b[39mparam_step, start_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised.py:315\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, param_range, param_step, ensemble_id, start_iter)\u001b[0m\n\u001b[1;32m    309\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# NOTE: the system size is constant for this inner loop but must be reset\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# here because leaving it out (or, equivalently, setting it to None) would\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# cause the model to sample a random system size. See set_param in model.py.\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m dummy_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(H\u001b[38;5;241m.\u001b[39mbasis, compute_phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRan forward for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m spins at point \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# loss = self.calculate_mse_step(\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m#     H, param, basis_batch=None, use_symmetry=True\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/model.py:309\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, spins, compute_phase)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# src_i = src_i + self.pos_embedding[:len(src_i)]  # (seq, batch, embedding)\u001b[39;00m\n\u001b[1;32m    308\u001b[0m src_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src_i, system_size)  \u001b[38;5;66;03m# (seq, batch, embedding)\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m output_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(\n\u001b[1;32m    310\u001b[0m     src_i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_mask\n\u001b[1;32m    311\u001b[0m )  \u001b[38;5;66;03m# (seq, batch, embedding)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m psi_output \u001b[38;5;241m=\u001b[39m output_i[\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_prefix_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m :\n\u001b[1;32m    314\u001b[0m ]  \u001b[38;5;66;03m# only use the physical degrees of freedom\u001b[39;00m\n\u001b[1;32m    315\u001b[0m amp_i \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_head(psi_output), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    317\u001b[0m )  \u001b[38;5;66;03m# (seq, batch, phys_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, src_mask\u001b[38;5;241m=\u001b[39mmask, is_causal\u001b[38;5;241m=\u001b[39mis_causal, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/custom_transformer_layer.py:119\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    120\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Projects/tqs/custom_transformer_layer.py:128\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    129\u001b[0m         x,\n\u001b[1;32m    130\u001b[0m         x,\n\u001b[1;32m    131\u001b[0m         x,\n\u001b[1;32m    132\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    133\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    134\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    135\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/tqs/custom_modules.py:142\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     87\u001b[0m             query: Tensor,\n\u001b[1;32m     88\u001b[0m             key: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m             attn_mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m             average_attn_weights: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[Tensor]]:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mNote::\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m      head of shape :math:`(N, num_heads, L, S)`.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(query, key, value, key_padding_mask,\n\u001b[1;32m    143\u001b[0m                               need_weights, attn_mask, average_attn_weights)\n",
      "File \u001b[0;32m~/Projects/tqs/custom_modules.py:259\u001b[0m, in \u001b[0;36mMultiheadAttention._forward_impl\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m         key_padding_mask \u001b[38;5;241m=\u001b[39m nnF\u001b[38;5;241m.\u001b[39mpad(key_padding_mask, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 259\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(attn_output_weights\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m [bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len]\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# with cProfile.Profile() as pr:\n",
    "# with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "opt.train(epochs=epochs, param_range=param_range, param_step=param_step, start_iter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
